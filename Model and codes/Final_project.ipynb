{"cells":[{"cell_type":"markdown","metadata":{"_id":"88B674BC40F1484B8256B9435C8D30F0","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E33FF46F89E348B3A7373F47CA3A0EF8","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 导包+数据简单处理"},{"cell_type":"code","metadata":{"_id":"9797294817FD4E1DB0AA079B35FF2A0F","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CABC335ACA434FEAA4BE92F71BD5EBD0","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"pip install category_encoders","outputs":[{"output_type":"stream","name":"stdout","text":"Collecting category_encoders\n  Downloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n     |████████████████████████████████| 82 kB 147 kB/s            \n\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (1.7.1)\nRequirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (0.5.2)\nRequirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (1.3.4)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from category_encoders) (5.4.0)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (1.19.5)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (1.0.1)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (0.13.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.5->category_encoders) (2021.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.0.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\nRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources->category_encoders) (3.6.0)\nInstalling collected packages: category-encoders\nSuccessfully installed category-encoders-2.6.4\nNote: you may need to restart the kernel to use updated packages.\n"}],"execution_count":1},{"cell_type":"code","metadata":{"id":"74D1647E79A244EDA857F137B2F21AFB","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"pip install optuna","outputs":[{"output_type":"stream","name":"stdout","text":"Collecting optuna\n  Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n     |████████████████████████████████| 362 kB 1.5 MB/s            \n\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna) (5.4.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.19.5)\nCollecting colorlog\n  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (21.2)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.7.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.62.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.26)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (1.1.5)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (5.4.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (4.8.1)\nRequirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (2.4.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.2)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (4.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.6.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\nInstalling collected packages: colorlog, optuna\nSuccessfully installed colorlog-6.9.0 optuna-4.0.0\nNote: you may need to restart the kernel to use updated packages.\n"}],"execution_count":2},{"cell_type":"code","metadata":{"id":"24152039B41B456C960AFCC677BA1CDC","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"pip install xgboost","outputs":[{"output_type":"stream","name":"stdout","text":"Collecting xgboost\n  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n     |████████████████████████████████| 255.9 MB 73 kB/s              █████▍                         | 50.7 MB 753 kB/s eta 0:04:33\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.19.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.7.1)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-1.6.2\nNote: you may need to restart the kernel to use updated packages.\n"}],"execution_count":3},{"cell_type":"code","metadata":{"_id":"9C7FA8ACBD3C48429D75DF312200415D","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"79428D816B1947B9A043D67B08CBFE29","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"import pandas as pd\nimport numpy as np\nimport re\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.feature_selection import SelectFromModel, RFE\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import KFold\nimport seaborn as sns\nimport matplotlib.pyplot as plt","outputs":[],"execution_count":4},{"cell_type":"code","metadata":{"_id":"7830E8F9FDDF4219A9112B7285BBEDEF","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"0A8AC60BF5E6467380297AAA558EEE30","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"train = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_train.csv')\npredict = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_test.csv')\nfullData = pd.concat([train,predict],axis=0)\nxiaoqu = pd.read_csv(\"/home/mw/project/xiaoqu_rent.csv\")","outputs":[],"execution_count":5},{"cell_type":"code","metadata":{"_id":"D5230B71A2AD4AEE9CE6F289B80AC40E","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"BE751256FAC7408398EBA8D37B301C2D","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"is_matching = fullData['小区名称'].isin(xiaoqu['名称'])\n\n# 获取匹配的索引\nmatching_indexes = fullData.index[is_matching]\n\nnot_matching = ~fullData['小区名称'].isin(xiaoqu['名称'])\n# 获取不匹配的索引\nunmatching_indexes = fullData.index[not_matching]\n\n# 获取不匹配的小区名称\nunmatched_names = fullData.loc[not_matching, '小区名称']","outputs":[],"execution_count":6},{"cell_type":"code","metadata":{"_id":"5212ACDFFF80470BA1BA55BFB4643DB7","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"7AAED35026884282801B0746986EAFEA","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#按照最近的经纬度匹配小区\ndef haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    计算两个经纬度点之间的距离（单位：千米）\n    \"\"\"\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    return km\n\n    # 对于匹配的索引的行，直接添加匹配的小区名称\nfullData.loc[matching_indexes, '匹配小区'] = fullData.loc[matching_indexes, '小区名称']\n\n# 对于不匹配的索引的行，根据经纬度匹配最近的 xiaoqu 的名称\nfor index, row in fullData.loc[unmatching_indexes].iterrows():\n    distances = haversine(row['lon'], row['lat'], xiaoqu['coord_x'], xiaoqu['coord_y'])\n    nearest_index = np.argmin(distances)\n    fullData.loc[index, '匹配小区'] = xiaoqu.loc[nearest_index, '名称']","outputs":[],"execution_count":7},{"cell_type":"code","metadata":{"_id":"8D06749368CF412F93075576C9C3DEBD","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E5D7BC2BF7E84BAC850E3E4B1FA93BCE","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"columns_to_merge = ['建筑年代','房屋总数','楼栋总数','绿 化 率', '容 积 率','物 业 费','停车位','停车费用','环线位置','平均每平米月租金']\nfullData = fullData.merge(xiaoqu[['名称'] + columns_to_merge],\n                          left_on='匹配小区', right_on='名称', how='left')\n\n#补充环线信息\nfullData['环线'] = fullData['环线'].combine_first(fullData['环线'])\nfullData['环线'].fillna(\"无\", inplace=True)\n\n# 删除多余的“名称”列\nfullData = fullData.drop(columns=['名称', '环线位置'])\nfullData.head()","outputs":[{"output_type":"execute_result","data":{"text/plain":"   城市    区域     板块    环线     小区名称          价格      房屋户型        所在楼层     建筑面积  \\\n0   0  79.0  111.0  二至三环    人定湖西里   6564200.0  2室1厅1厨1卫   中楼层 (共5层)    52.3㎡   \n1   0  43.0  231.0  五至六环    龙跃苑四区   4174000.0  3室1厅1厨1卫    顶层 (共6层)  127.44㎡   \n2   0  97.0   54.0  五至六环      名都园  16310000.0  4室2厅1厨4卫    底层 (共3层)  228.54㎡   \n3   0  62.0  568.0  三至四环   保利海德公园   2834600.0     2房间2卫  低楼层 (共10层)    43.6㎡   \n4   0  62.0  226.0  三至四环  京投银泰琨御府   1954000.0     1房间1卫  中楼层 (共10层)   39.85㎡   \n\n     套内面积  ...     匹配小区    建筑年代  房屋总数 楼栋总数 绿 化 率 容 积 率          物 业 费     停车位  \\\n0     NaN  ...    人定湖西里  1982.0  1317   19  30.0  3.00  1.3-1.65元/月/㎡   300.0   \n1  123.7㎡  ...    龙跃苑四区  2005.0  2317   40  30.0  1.73      0.65元/月/㎡  1550.0   \n2     NaN  ...      名都园  2002.0  1249  565  30.1  0.64      2-10元/月/㎡     0.0   \n3  29.39㎡  ...   保利海德公园  2015.0   577   12  40.0  2.60   6.18-20元/月/㎡   950.0   \n4  29.94㎡  ...  京投银泰琨御府  2010.0  1685   19  60.0  1.58   4.8-5.5元/月/㎡  1800.0   \n\n     停车费用    平均每平米月租金  \n0     0.0  143.621275  \n1   150.0   65.334173  \n2     0.0  123.502865  \n3  1150.0  234.292104  \n4  1200.0  172.492516  \n\n[5 rows x 42 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>城市</th>\n      <th>区域</th>\n      <th>板块</th>\n      <th>环线</th>\n      <th>小区名称</th>\n      <th>价格</th>\n      <th>房屋户型</th>\n      <th>所在楼层</th>\n      <th>建筑面积</th>\n      <th>套内面积</th>\n      <th>...</th>\n      <th>匹配小区</th>\n      <th>建筑年代</th>\n      <th>房屋总数</th>\n      <th>楼栋总数</th>\n      <th>绿 化 率</th>\n      <th>容 积 率</th>\n      <th>物 业 费</th>\n      <th>停车位</th>\n      <th>停车费用</th>\n      <th>平均每平米月租金</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>79.0</td>\n      <td>111.0</td>\n      <td>二至三环</td>\n      <td>人定湖西里</td>\n      <td>6564200.0</td>\n      <td>2室1厅1厨1卫</td>\n      <td>中楼层 (共5层)</td>\n      <td>52.3㎡</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>人定湖西里</td>\n      <td>1982.0</td>\n      <td>1317</td>\n      <td>19</td>\n      <td>30.0</td>\n      <td>3.00</td>\n      <td>1.3-1.65元/月/㎡</td>\n      <td>300.0</td>\n      <td>0.0</td>\n      <td>143.621275</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>43.0</td>\n      <td>231.0</td>\n      <td>五至六环</td>\n      <td>龙跃苑四区</td>\n      <td>4174000.0</td>\n      <td>3室1厅1厨1卫</td>\n      <td>顶层 (共6层)</td>\n      <td>127.44㎡</td>\n      <td>123.7㎡</td>\n      <td>...</td>\n      <td>龙跃苑四区</td>\n      <td>2005.0</td>\n      <td>2317</td>\n      <td>40</td>\n      <td>30.0</td>\n      <td>1.73</td>\n      <td>0.65元/月/㎡</td>\n      <td>1550.0</td>\n      <td>150.0</td>\n      <td>65.334173</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>97.0</td>\n      <td>54.0</td>\n      <td>五至六环</td>\n      <td>名都园</td>\n      <td>16310000.0</td>\n      <td>4室2厅1厨4卫</td>\n      <td>底层 (共3层)</td>\n      <td>228.54㎡</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>名都园</td>\n      <td>2002.0</td>\n      <td>1249</td>\n      <td>565</td>\n      <td>30.1</td>\n      <td>0.64</td>\n      <td>2-10元/月/㎡</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>123.502865</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>62.0</td>\n      <td>568.0</td>\n      <td>三至四环</td>\n      <td>保利海德公园</td>\n      <td>2834600.0</td>\n      <td>2房间2卫</td>\n      <td>低楼层 (共10层)</td>\n      <td>43.6㎡</td>\n      <td>29.39㎡</td>\n      <td>...</td>\n      <td>保利海德公园</td>\n      <td>2015.0</td>\n      <td>577</td>\n      <td>12</td>\n      <td>40.0</td>\n      <td>2.60</td>\n      <td>6.18-20元/月/㎡</td>\n      <td>950.0</td>\n      <td>1150.0</td>\n      <td>234.292104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>62.0</td>\n      <td>226.0</td>\n      <td>三至四环</td>\n      <td>京投银泰琨御府</td>\n      <td>1954000.0</td>\n      <td>1房间1卫</td>\n      <td>中楼层 (共10层)</td>\n      <td>39.85㎡</td>\n      <td>29.94㎡</td>\n      <td>...</td>\n      <td>京投银泰琨御府</td>\n      <td>2010.0</td>\n      <td>1685</td>\n      <td>19</td>\n      <td>60.0</td>\n      <td>1.58</td>\n      <td>4.8-5.5元/月/㎡</td>\n      <td>1800.0</td>\n      <td>1200.0</td>\n      <td>172.492516</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 42 columns</p>\n</div>"},"metadata":{},"execution_count":8}],"execution_count":8},{"cell_type":"code","metadata":{"_id":"F37E0AD6D1414DB29F0E6C0656D46DC5","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"99522A80DFC940C8B14DAC5E2FF02F20","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"df = fullData.loc[:84132]\npredict_data = fullData.loc[84133:]","outputs":[],"execution_count":9},{"cell_type":"code","metadata":{"_id":"F0A9309F5E434C1A9DB3258E81C98C01","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"DFF401D67D7E449EA2469439E61422CB","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"print(\"合并后行数:\", len(predict_data))\ntrain_df, test_df = train_test_split(df, test_size=0.15, random_state=111)","outputs":[{"output_type":"stream","name":"stdout","text":"合并后行数: 14786\n"}],"execution_count":10},{"cell_type":"code","metadata":{"_id":"A730D417F64F4A4DA1F6042F948583F9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E39D8DAED833401F8493461D4FDE0EF4","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"xiaoqu.info()","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2973 entries, 0 to 2972\nData columns (total 21 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   名称        2973 non-null   object \n 1   环线位置      1730 non-null   object \n 2   建筑年代      2793 non-null   float64\n 3   房屋总数      2973 non-null   int64  \n 4   楼栋总数      2973 non-null   int64  \n 5   物业公司      2344 non-null   object \n 6   绿 化 率     2973 non-null   float64\n 7   容 积 率     2973 non-null   float64\n 8   物 业 费     2630 non-null   object \n 9   供水        2798 non-null   object \n 10  供暖        2273 non-null   object \n 11  供电        2812 non-null   object \n 12  燃气费       2564 non-null   object \n 13  供热费       1190 non-null   object \n 14  停车位       2973 non-null   float64\n 15  停车费用      2973 non-null   float64\n 16  coord_x   2973 non-null   float64\n 17  coord_y   2973 non-null   float64\n 18  匹配租房      2973 non-null   object \n 19  小区名称      2973 non-null   object \n 20  平均每平米月租金  2973 non-null   float64\ndtypes: float64(8), int64(2), object(11)\nmemory usage: 487.9+ KB\n"}],"execution_count":11},{"cell_type":"code","metadata":{"id":"51A3390F074F40DCB9F5D835E68DD371","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, PolynomialFeatures\n\n\ndef preprocess_full(df, is_training=True, encoders=None, transformers=None, use_poly=True):\n    \"\"\"\n    统一处理分类变量、数值变量、文本提取等特征工程操作。\n    \n    参数:\n        df: 待处理的DataFrame\n        is_training: 是否为训练阶段\n        encoders: 分类变量的编码器（如 OneHotEncoder）\n        transformers: 数值变量的变换器（如 imputer、scaler、poly）\n        use_poly: 是否对数值变量做多项式扩展\n\n    返回:\n        - 训练集: df, encoders, transformers\n        - 测试集: df\n    \"\"\"\n    df = df.copy()\n\n    # 1. 删除重复与无效列\n    df = df.drop_duplicates().dropna(axis=1, how='all')\n    drop_cols = ['交易权属','年份', '环线位置', '小区地址', '物业类别',\n                 '上次交易', '房屋用途', '房屋年限', '产权所属', '套内面积', '别墅类型']\n    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n\n    # 2. 时间处理\n    if '交易时间' in df.columns:\n        df['交易时间'] = pd.to_datetime(df['交易时间'], errors='coerce')\n        df['交易年份'] = df['交易时间'].dt.year\n        df.drop(columns=['交易时间'], inplace=True)\n\n    # 3. 填充字段 & 编码处理\n    if '装修情况' in df.columns:\n        df['装修情况'] = df['装修情况'].fillna('其他')\n        df['装修编码'] = df['装修情况'].map({'简装': '简装', '精装': '精装', '毛坯': '毛坯'}).fillna('其他')\n    else:\n        df['装修编码'] = '其他'\n\n    if '配备电梯' in df.columns:\n        df['配备电梯'] = df['配备电梯'].fillna('无').map({'有': 1, '无': 0})\n    else:\n        df['配备电梯'] = 0\n\n    if '建筑结构' in df.columns:\n        df['建筑结构'] = df['建筑结构'].fillna('钢混结构')\n        df['建筑结构编码'] = df['建筑结构'].map({'混合结构': '混合结构', '钢混结构': '钢混结构'}).fillna('其他')\n    else:\n        df['建筑结构编码'] = '其他'\n\n    # 4. OneHot 编码\n    onehot_cols = ['建筑结构编码', '装修编码']\n    existing_cols = [col for col in onehot_cols if col in df.columns]\n\n    if is_training:\n        encoders = {} if encoders is None else encoders\n        encoder = OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse=False)\n        encoders['onehot_encoder'] = encoder\n\n        if existing_cols:\n            onehot_encoded = encoder.fit_transform(df[existing_cols])\n            col_names = encoder.get_feature_names_out(existing_cols)\n            onehot_df = pd.DataFrame(onehot_encoded, columns=col_names, index=df.index)\n            df.drop(columns=existing_cols, inplace=True)\n            df = pd.concat([df, onehot_df], axis=1)\n    else:\n        if existing_cols and encoders and 'onehot_encoder' in encoders:\n            encoder = encoders['onehot_encoder']\n            onehot_encoded = encoder.transform(df[existing_cols])\n            col_names = encoder.get_feature_names_out(existing_cols)\n            onehot_df = pd.DataFrame(onehot_encoded, columns=col_names, index=df.index)\n            df.drop(columns=existing_cols, inplace=True)\n            df = pd.concat([df, onehot_df], axis=1)\n\n    # 5. 文本字段数值提取\n    def safe_extract_digits(x):\n        if pd.isna(x):\n            return np.nan\n        matches = re.findall(r'\\d+\\.?\\d*', str(x))\n        return float(matches[0]) if matches else np.nan\n\n    numeric_text_cols = {\n        '房屋总数': '房屋数量',\n        '楼栋总数': '楼栋数量',\n        '绿 化 率': '绿化率',\n        '容 积 率': '容积率',\n        '物 业 费': '物业费',\n        '停车位': '停车位数量',\n        '停车费用': '停车费'\n    }\n\n    for col, new_col in numeric_text_cols.items():\n        if col in df.columns:\n            df[new_col] = df[col].apply(safe_extract_digits)\n\n    numerical_cols = ['房屋数量', '楼栋数量', '绿化率', '容积率', '物业费', '停车位数量', '停车费']\n    X_num = df[numerical_cols].copy()\n\n    # 6. 数值变量处理\n    # 6.1 缺失值填补\n    if is_training:\n        imputer = SimpleImputer(strategy='mean')\n        X_num_imputed = imputer.fit_transform(X_num)\n        transformers = transformers or {}\n        transformers['imputer'] = imputer\n    else:\n        imputer = transformers['imputer']\n        X_num_imputed = imputer.transform(X_num)\n\n    # 6.2 裁剪离群值\n    def clip_outliers(X):\n        lower = np.percentile(X, 5, axis=0)\n        upper = np.percentile(X, 95, axis=0)\n        return np.clip(X, lower, upper)\n\n    X_clipped = clip_outliers(X_num_imputed)\n\n    # 6.3 log1p 变换\n    X_log = np.log1p(np.clip(X_clipped, a_min=0, a_max=None))\n\n    # 6.4 标准化\n    if is_training:\n        scaler = RobustScaler()\n        X_scaled = scaler.fit_transform(X_log)\n        transformers['scaler'] = scaler\n    else:\n        scaler = transformers['scaler']\n        X_scaled = scaler.transform(X_log)\n\n    # 6.5 多项式扩展（可选）\n    if use_poly:\n        if is_training:\n            poly = PolynomialFeatures(degree=2, include_bias=False)\n            X_poly = poly.fit_transform(X_scaled)\n            transformers['poly'] = poly\n            col_names = [f'poly_{name}' for name in poly.get_feature_names_out(numerical_cols)]\n        else:\n            poly = transformers['poly']\n            X_poly = poly.transform(X_scaled)\n            col_names = [f'poly_{name}' for name in poly.get_feature_names_out(numerical_cols)]\n\n        X_num_df = pd.DataFrame(X_poly, columns=col_names, index=df.index)\n    else:\n        X_num_df = pd.DataFrame(X_scaled, columns=numerical_cols, index=df.index)\n\n    # 删除冲突列并拼接\n    df.drop(columns=df.columns.intersection(X_num_df.columns), inplace=True)\n    df = pd.concat([df, X_num_df], axis=1)\n\n    # 最后删除原始分类字段\n    df.drop(columns=['建筑结构', '装修情况'], errors='ignore', inplace=True)\n\n    if is_training:\n        return df, encoders, transformers\n    else:\n        return df","outputs":[],"execution_count":12},{"cell_type":"code","metadata":{"id":"1C2BE43D33CA479B9DE5CE6DF1CD1428","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"train_df, encoders,transformers = preprocess_full(train_df, is_training=True)\ntest_df = preprocess_full(test_df, is_training=False, encoders=encoders, transformers=transformers)\npredict_data = preprocess_full(predict_data, is_training=False, encoders=encoders, transformers=transformers)","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{"_id":"AEA7A02EB70D4C19BC352CE40F928BB5","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"9A9C1E30A23649139565F22FE7742B25","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 建筑面积"},{"cell_type":"code","metadata":{"_id":"AFE8587B0C8D4AED98E96572CD48E32E","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"B292F6A06FEC4E099CEFECC229FD8B62","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"def clean_area_column(df):\n    \"\"\"清洗‘建筑面积’列的非数字字符并转换为浮点数\"\"\"\n    df['建筑面积'] = (\n        df['建筑面积']\n        .astype(str)\n        .str.replace(r'[^0-9.]', '', regex=True)\n        .astype(float)\n    )\n    return df\n\n# 分别应用到每个数据集\ntrain_df = clean_area_column(train_df)\ntest_df = clean_area_column(test_df)\npredict_data = clean_area_column(predict_data)\n\n# Step 2. 数据验证\n# 检查转换结果\nprint(\"清洗后示例：\\n\", train_df['建筑面积'].sample(5))\nprint(\"\\n异常值统计：\")\nprint(\"零或负面积:\", train_df[train_df['建筑面积'] <=0].shape[0])\nprint(\"缺失值比例:\", train_df['建筑面积'].isnull().mean())\nprint(\"建筑面积描述统计：\\n\", train_df['建筑面积'].describe())","outputs":[{"output_type":"stream","name":"stdout","text":"清洗后示例：\n 16211    130.65\n36069     89.46\n50959     95.58\n16767     62.44\n11874     57.24\nName: 建筑面积, dtype: float64\n\n异常值统计：\n零或负面积: 0\n缺失值比例: 0.0\n建筑面积描述统计：\n count    71458.000000\nmean        96.641668\nstd         65.316834\nmin         10.000000\n25%         66.000000\n50%         88.870000\n75%        115.960000\nmax      10337.000000\nName: 建筑面积, dtype: float64\n"}],"execution_count":14},{"cell_type":"code","metadata":{"_id":"A09CC4D570A14C4DAA9F0FD720667FB0","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"BAF4F88073874DC98DC42FA7A67404FB","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#计算99%分位数作为截断点\narea_99_percentile = train_df['建筑面积'].quantile(0.99)\n\n#截断异常值\ntrain_df.loc[train_df['建筑面积'] > area_99_percentile, '建筑面积'] = area_99_percentile\ntest_df.loc[test_df['建筑面积'] > area_99_percentile, '建筑面积'] = area_99_percentile\npredict_data.loc[predict_data['建筑面积'] > area_99_percentile, '建筑面积'] = area_99_percentile\n\n# 创建管道：先标准化，再添加多项式特征\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n])\n    \n# 仅选择建筑面积列\narea_train = train_df[[\"建筑面积\"]]\narea_test = test_df[[\"建筑面积\"]]\narea_predict=predict_data[[\"建筑面积\"]]\n    \n# 转换训练集\narea_poly_train = pipeline.fit_transform(area_train)\n# 使用训练集的均值和标准差转换测试集\narea_poly_test = pipeline.transform(area_test)\narea_poly_predict = pipeline.transform(area_predict)\n    \n# 将转换后的特征添加回原始数据框\ntrain_df = train_df.copy()\ntrain_df[\"area_scaled\"] = area_poly_train[:, 0]\ntrain_df[\"area_squared\"] = area_poly_train[:, 1]\n    \ntest_df = test_df.copy()\ntest_df[\"area_scaled\"] = area_poly_test[:, 0]\ntest_df[\"area_squared\"] = area_poly_test[:, 1]\n    \npredict_data = predict_data.copy()\npredict_data[\"area_scaled\"] = area_poly_predict[:, 0]\npredict_data[\"area_squared\"] = area_poly_predict[:, 1]","outputs":[],"execution_count":15},{"cell_type":"code","metadata":{"_id":"DBEDBC3C05A141EB86569488AE173E51","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"1533A877ADCD42498E4E388D5BC801DD","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 检查数据集\nprint(\"当前所有列名:\", train_df.columns.tolist())","outputs":[{"output_type":"stream","name":"stdout","text":"当前所有列名: ['城市', '区域', '板块', '环线', '小区名称', '价格', '房屋户型', '所在楼层', '建筑面积', '房屋朝向', '梯户比例', '配备电梯', '房屋优势', '核心卖点', '户型介绍', '周边配套', '交通出行', 'lon', 'lat', '匹配小区', '建筑年代', '房屋总数', '楼栋总数', '绿 化 率', '容 积 率', '物 业 费', '停车位', '停车费用', '平均每平米月租金', '交易年份', '建筑结构编码_混合结构', '建筑结构编码_钢混结构', '装修编码_毛坯', '装修编码_简装', '装修编码_精装', '房屋数量', '楼栋数量', '绿化率', '容积率', '物业费', '停车位数量', '停车费', 'poly_房屋数量', 'poly_楼栋数量', 'poly_绿化率', 'poly_容积率', 'poly_物业费', 'poly_停车位数量', 'poly_停车费', 'poly_房屋数量^2', 'poly_房屋数量 楼栋数量', 'poly_房屋数量 绿化率', 'poly_房屋数量 容积率', 'poly_房屋数量 物业费', 'poly_房屋数量 停车位数量', 'poly_房屋数量 停车费', 'poly_楼栋数量^2', 'poly_楼栋数量 绿化率', 'poly_楼栋数量 容积率', 'poly_楼栋数量 物业费', 'poly_楼栋数量 停车位数量', 'poly_楼栋数量 停车费', 'poly_绿化率^2', 'poly_绿化率 容积率', 'poly_绿化率 物业费', 'poly_绿化率 停车位数量', 'poly_绿化率 停车费', 'poly_容积率^2', 'poly_容积率 物业费', 'poly_容积率 停车位数量', 'poly_容积率 停车费', 'poly_物业费^2', 'poly_物业费 停车位数量', 'poly_物业费 停车费', 'poly_停车位数量^2', 'poly_停车位数量 停车费', 'poly_停车费^2', 'area_scaled', 'area_squared']\n"}],"execution_count":16},{"cell_type":"markdown","metadata":{"_id":"841FC23E05D1401FB169D3F53756BC35","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"56CD9FB951ED4CD58748BCD38B4CC357","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 房屋朝向"},{"cell_type":"code","metadata":{"_id":"219545D03ED74F54B10B836AC227BDC9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"44F629634B8D495AB341B7D1C48AAD19","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#房屋朝向处理\ndef extract_main_direction(orientation):\n    \"\"\"\n    规则优先级：南 > 北 > 东 > 西\n    参数：\n        orientation: 字符串（如\"东南北\"）或NaN\n    返回：\n        主要朝向（\"南\",\"北\",\"东\",\"西\",\"其他\"）\n    \"\"\"\n    if pd.isna(orientation):\n        return \"其他\"\n    orientation = str(orientation).replace(\" \", \"\")  # 清洗空格\n    \n    # 按优先级判断\n    if \"南\" in orientation:\n        return \"南\"\n    elif \"北\" in orientation:\n        return \"北\"\n    elif \"东\" in orientation:\n        return \"东\"\n    elif \"西\" in orientation:\n        return \"西\"\n    else:\n        return \"其他\"\n\n# 训练集处理\ntrain_df[\"主要朝向\"] = train_df[\"房屋朝向\"].apply(extract_main_direction)\n\n# 验证集处理（必须使用相同处理方式）\ntest_df[\"主要朝向\"] = test_df[\"房屋朝向\"].apply(extract_main_direction)\n\n# 预测集处理\npredict_data[\"主要朝向\"] = predict_data[\"房屋朝向\"].apply(extract_main_direction)\n        \n# 初始化编码器（自动处理未知类别）\nencoder = OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse=False)\nencoder.fit(train_df[[\"主要朝向\"]])  # 只在训练集上fit\n\n# 训练集\ntrain_encoded = encoder.transform(train_df[[\"主要朝向\"]])\ntrain_df = pd.concat([\n    train_df.drop([\"房屋朝向\", \"主要朝向\"], axis=1),\n    pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out([\"主要朝向\"]), index=train_df.index)\n], axis=1)\n\n# 验证集\ntest_encoded = encoder.transform(test_df[[\"主要朝向\"]])\ntest_df = pd.concat([\n    test_df.drop([\"房屋朝向\", \"主要朝向\"], axis=1),\n    pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out([\"主要朝向\"]), index=test_df.index)\n], axis=1)\n\n# 预测集\npred_encoded = encoder.transform(predict_data[[\"主要朝向\"]])\npredict_data = pd.concat([\n    predict_data.drop([\"房屋朝向\", \"主要朝向\"], axis=1),\n    pd.DataFrame(pred_encoded, columns=encoder.get_feature_names_out([\"主要朝向\"]), index=predict_data.index)\n], axis=1)","outputs":[],"execution_count":17},{"cell_type":"markdown","metadata":{"_id":"F78335A4F6BA46AD9F3540FA826F2228","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"6D0B2EBBFAC14E4DADCEB8C83C626725","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 城市_环线"},{"cell_type":"code","metadata":{"_id":"EBCA185FD51F4BC793F43B1E2E94277C","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CBDAC43694A34EFA885988C2E3A4B92E","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"print(train_df['环线'].isnull().sum())  # 缺失数量\nprint(train_df.groupby('城市')['环线'].apply(lambda x: x.isnull().mean()))  # 每个城市的缺失比例\ncity_region_ring_counts = train_df.groupby(['城市', '板块'])['环线'].nunique().reset_index()\ncity_region_ring_counts.rename(columns={'环线': '环线类别数'}, inplace=True)\n# 筛选环线类别数 > 2 的 (城市, 板块)\ncity_region_unstable = city_region_ring_counts[city_region_ring_counts['环线类别数'] > 2]\n\n# 显示结果\nprint(city_region_unstable)\n\n# 统计这些 (城市, 板块) 组合的个数\nnum_unstable_regions = city_region_unstable.shape[0]\nprint(f\"环线类别数大于 2 的 (城市, 板块) 组合个数: {num_unstable_regions}\")","outputs":[{"output_type":"stream","name":"stdout","text":"0\n城市\n0    0.0\n1    0.0\n2    0.0\n3    0.0\n4    0.0\n5    0.0\n6    0.0\nName: 环线, dtype: float64\n     城市     板块  环线类别数\n0     0    1.0      3\n56    0  226.0      3\n177   0  686.0      3\n188   0  749.0      3\n229   2   70.0      3\n235   2  122.0      3\n254   2  246.0      3\n257   2  269.0      3\n268   2  376.0      3\n323   2  804.0      3\n325   2  809.0      3\n329   3   18.0      3\n337   3   56.0      3\n354   3  152.0      3\n466   3  699.0      3\n476   3  732.0      3\n490   4    6.0      3\n492   4   22.0      3\n495   4   50.0      3\n497   4   74.0      3\n498   4  102.0      3\n508   4  192.0      3\n509   4  195.0      3\n518   4  273.0      3\n520   4  314.0      3\n525   4  366.0      4\n532   4  462.0      3\n535   4  466.0      3\n539   4  476.0      3\n541   4  486.0      3\n542   4  489.0      3\n543   4  496.0      3\n544   4  538.0      3\n546   4  555.0      3\n556   4  649.0      3\n557   4  700.0      3\n559   4  704.0      3\n562   4  726.0      3\n563   4  738.0      3\n564   4  741.0      3\n566   4  793.0      4\n622   6  238.0      3\n623   6  242.0      3\n624   6  244.0      3\n625   6  245.0      3\n627   6  250.0      3\n631   6  407.0      3\n634   6  498.0      3\n640   6  597.0      3\n647   6  706.0      3\n648   6  710.0      3\n环线类别数大于 2 的 (城市, 板块) 组合个数: 51\n"}],"execution_count":18},{"cell_type":"code","metadata":{"_id":"1CC4A4A489DD400C87A3B225ED8FF60B","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"201951E264844C4393BB05F7A0AD4135","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"def train_fill_missing_环线(train_df):\n    \"\"\"仅在训练集计算填补规则\"\"\"\n   \n    # 规则1: 城市1和5的环线固定为\"无环线\"\n    city_special_rules = {1 : '无环线', 5 : '无环线'}\n   \n    # 规则2: 按板块→区域→城市层级计算环线众数\n    板块_环线_map = train_df.groupby(\"板块\")[\"环线\"].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan).to_dict()\n    区域_环线_map = train_df.groupby(\"区域\")[\"环线\"].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan).to_dict()\n    城市_环线_map = train_df.groupby(\"城市\")[\"环线\"].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan).to_dict()\n   \n    return {\n        'city_special_rules': city_special_rules,\n        '板块_环线_map': 板块_环线_map,\n        '区域_环线_map': 区域_环线_map,\n        '城市_环线_map': 城市_环线_map\n    }\n\ndef apply_fill_missing_环线(df, fill_rules):\n    \"\"\"应用填补规则到任意数据集\"\"\"\n   \n    # 深拷贝避免修改原数据\n    df = df.copy()\n   \n    # 应用特殊城市规则\n    special_cities = fill_rules['city_special_rules'].keys()\n    df.loc[df[\"城市\"].astype(int).isin(special_cities), \"环线\"] = df[\"城市\"].map(fill_rules['city_special_rules'])\n   \n    # 按层级填补\n    df[\"环线_填补\"] = (\n        df[\"环线\"]\n        .fillna(df[\"板块\"].map(fill_rules['板块_环线_map']))\n        .fillna(df[\"区域\"].map(fill_rules['区域_环线_map']))\n        .fillna(df[\"城市\"].map(fill_rules['城市_环线_map']))\n    )\n   \n    return df\n\n# 只在训练集计算填补规则\nfill_rules = train_fill_missing_环线(train_df)\n\n# 应用规则到所有数据集\ntrain_df = apply_fill_missing_环线(train_df, fill_rules)\ntest_df = apply_fill_missing_环线(test_df, fill_rules)\npredict_data = apply_fill_missing_环线(predict_data, fill_rules)\n\n# 创建城市_环线变量\nfor df in [train_df, test_df, predict_data]:\n    df[\"城市_环线\"] = df[\"城市\"].astype(str) + \"_\" + df[\"环线_填补\"].astype(str)","outputs":[],"execution_count":19},{"cell_type":"code","metadata":{"_id":"1ABD5F7309294FFB8D0BB51FE691680C","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"B31AA7077F6C4475852FE9A7F79CE246","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#绘图-城市_环线与建筑面积\nplt.figure(figsize=(12, 8))\nsns.lmplot(\n    x='建筑面积',\n    y='价格',\n    hue='城市_环线',\n    data=train_df,\n    height=6,\n    aspect=1.5,\n    scatter_kws={'alpha': 0.3},  # 设置点透明度\n    line_kws={'linewidth': 2}    # 设置回归线粗细\n)\nplt.title('建筑面积 vs 价格（按城市环线分组）', fontsize=14)\nplt.xlabel('建筑面积 (㎡)', fontsize=12)\nplt.ylabel('价格', fontsize=12)\nplt.show()","outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x576 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 745.125x432 with 1 Axes>","text/html":"<img src=\"/s3/static-files/upload/rt/D9F182D5F9BB435997858E32AE2ED35E/sx6i751ehc.png\">"},"metadata":{"needs_background":"light"}}],"execution_count":20},{"cell_type":"code","metadata":{"_id":"D4599D9D08714FAF86EF67E07817BB87","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"54819C16DCFC4C93B68E4E8FE1B77861","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 初始化编码器（参数不变）\nencoder = OneHotEncoder(\n    sparse=True,\n    handle_unknown=\"ignore\",\n    drop=\"first\"\n)\n\n# 只在训练集拟合\nencoder.fit(train_df[[\"城市_环线\"]])\n\n# 转换所有数据集\ndef encode_city_ring(df, encoder):\n    encoded = encoder.transform(df[[\"城市_环线\"]])\n    \n    # 修改为兼容新版pandas的代码\n    try:\n        # 尝试使用老方法\n        encoded_df = pd.DataFrame.sparse.from_spmatrix(\n            encoded,\n            columns=encoder.get_feature_names_out([\"城市_环线\"]),\n            index=df.index\n        )\n    except AttributeError:\n        # 如果不支持，使用新方法\n        encoded_df = pd.DataFrame(\n            encoded.toarray(),\n            columns=encoder.get_feature_names_out([\"城市_环线\"]),\n            index=df.index\n        )\n    return pd.concat([df.drop(\"城市_环线\", axis=1), encoded_df], axis=1)\n\ntrain_df = encode_city_ring(train_df, encoder)\ntest_df = encode_city_ring(test_df, encoder)\npredict_data = encode_city_ring(predict_data, encoder)\n\n# 检查填补后缺失值\nfor name, df in zip([\"Train\", \"Test\", \"Predict\"], [train_df, test_df, predict_data]):\n    print(f\"{name}环线缺失数:\", df[\"环线_填补\"].isna().sum())\n\n# 查看编码特征维度（应全部相同）\nprint(\"编码特征:\", encoder.get_feature_names_out())","outputs":[{"output_type":"stream","name":"stdout","text":"Train环线缺失数: 0\nTest环线缺失数: 0\nPredict环线缺失数: 0\n编码特征: ['城市_环线_0_二环内' '城市_环线_0_二至三环' '城市_环线_0_五至六环' '城市_环线_0_六环外' '城市_环线_0_四至五环'\n '城市_环线_0_无' '城市_环线_1_无环线' '城市_环线_2_内环内' '城市_环线_2_内环至外环' '城市_环线_2_外环外'\n '城市_环线_2_无' '城市_环线_3_中环至外环' '城市_环线_3_内环内' '城市_环线_3_内环至中环' '城市_环线_3_外环外'\n '城市_环线_3_无' '城市_环线_4_一环内' '城市_环线_4_一至二环' '城市_环线_4_三至四环' '城市_环线_4_二至三环'\n '城市_环线_4_四环外' '城市_环线_4_无' '城市_环线_5_无环线' '城市_环线_6_一环内' '城市_环线_6_一至二环'\n '城市_环线_6_三环外' '城市_环线_6_二至三环' '城市_环线_6_无']\n"}],"execution_count":21},{"cell_type":"code","metadata":{"_id":"9E64CEC955AD4CE890B1C8EA6762B131","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"3A7004733E75448CBA402C3B81C4DC5E","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#创建交互项\ncity_ring_columns = [col for col in train_df.columns if col.startswith(\"城市_环线_\")]\ndef add_interaction_terms(df, city_ring_cols):\n    for col in city_ring_cols:\n        df[f\"{col}_建筑面积\"] = df[col] * df[\"建筑面积\"]\n    return df\n\ntrain_df = add_interaction_terms(train_df, city_ring_columns)\ntest_df = add_interaction_terms(test_df, city_ring_columns)\npredict_data = add_interaction_terms(predict_data, city_ring_columns)\n\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 仅标准化交互项\ninteraction_columns = [col for col in train_df.columns if \"城市_环线_\" in col and \"建筑面积\" in col]\n\n# 为每个交互项分别处理\nfor col in interaction_columns:\n    # 检查是否是稀疏数据并适当处理\n    scaler = StandardScaler(with_mean=False)\n    \n    # 训练集\n    if hasattr(train_df[col], 'sparse'):\n        # 如果是SparseArray类型\n        train_df[col] = scaler.fit_transform(np.array(train_df[col]).reshape(-1, 1))\n    elif isinstance(train_df[col].array, pd.arrays.SparseArray):\n        # 新版pandas中的稀疏数组处理\n        train_df[col] = scaler.fit_transform(train_df[col].array.to_numpy().reshape(-1, 1))\n    else:\n        # 常规数组处理\n        train_df[col] = scaler.fit_transform(np.array(train_df[col]).reshape(-1, 1))\n    \n    # 测试集 (同样的处理逻辑)\n    if hasattr(test_df[col], 'sparse'):\n        test_df[col] = scaler.transform(np.array(test_df[col]).reshape(-1, 1))\n    elif isinstance(test_df[col].array, pd.arrays.SparseArray):\n        test_df[col] = scaler.transform(test_df[col].array.to_numpy().reshape(-1, 1))\n    else:\n        test_df[col] = scaler.transform(np.array(test_df[col]).reshape(-1, 1))\n    \n    # 预测集 (同样的处理逻辑)\n    if hasattr(predict_data[col], 'sparse'):\n        predict_data[col] = scaler.transform(np.array(predict_data[col]).reshape(-1, 1))\n    elif isinstance(predict_data[col].array, pd.arrays.SparseArray):\n        predict_data[col] = scaler.transform(predict_data[col].array.to_numpy().reshape(-1, 1))\n    else:\n        predict_data[col] = scaler.transform(np.array(predict_data[col]).reshape(-1, 1))","outputs":[],"execution_count":22},{"cell_type":"markdown","metadata":{"_id":"A576258DD4A3486186D7E39B3C12D778","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"1E9EAC1776CC4848AC4002664CDF9CA0","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 梯户比例"},{"cell_type":"code","metadata":{"_id":"D061A66DE4854A24837155878B475731","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"4720B21A83AC4C8A9452EA441D857EAB","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"def process_elevator_household_ratio(df):\n    \"\"\"处理梯户比例字段\"\"\"\n    # 填充缺失值（用默认值 \"1梯2户\" 代替 \"无\"）\n    df['梯户比例'] = df['梯户比例'].fillna('1梯2户')\n    \n    def extract_ratio(ratio_str):\n        if not isinstance(ratio_str, str):\n            return np.nan\n        \n        # 更灵活的正则匹配（支持 \"2梯4户\", \"2部电梯4户\", \"2T4H\" 等）\n        match = re.search(r'(\\d+)\\s*[梯部T]\\s*(\\d+)\\s*户?', ratio_str)\n        if match:\n            elevators = int(match.group(1))\n            households = int(match.group(2))\n            return households / elevators if elevators > 0 else np.nan\n        return np.nan  # 如果无法解析，返回 NaN（后续会填充）\n    \n    # 计算户梯比\n    df['户梯比'] = df['梯户比例'].apply(extract_ratio)\n    \n    # 先按小区填充，再用全局中位数填充剩余的缺失值\n    global_median = df['户梯比'].median()\n    df['户梯比'] = df.groupby('小区名称')['户梯比'].transform(\n        lambda x: x.fillna(x.median() if not x.isnull().all() else global_median)\n    )\n    df['户梯比'] = df['户梯比'].fillna(global_median)\n    \n    return df\n\n# 处理数据\ntrain_df = process_elevator_household_ratio(train_df)\ntest_df = process_elevator_household_ratio(test_df)\npredict_data = process_elevator_household_ratio(predict_data)","outputs":[],"execution_count":23},{"cell_type":"code","metadata":{"id":"1A109F88B4CE46B48D69F8740472745F","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"print(\"处理后 train_df 的缺失值统计:\")\nprint(train_df['户梯比'].isna().sum())\n\nprint(\"\\n处理后 test_df 的缺失值统计:\")\nprint(test_df['户梯比'].isna().sum())\n\nprint(\"\\n处理后 predict_data 的缺失值统计:\")\nprint(predict_data['户梯比'].isna().sum())","outputs":[{"output_type":"stream","name":"stdout","text":"处理后 train_df 的缺失值统计:\n0\n\n处理后 test_df 的缺失值统计:\n0\n\n处理后 predict_data 的缺失值统计:\n0\n"}],"execution_count":24},{"cell_type":"markdown","metadata":{"_id":"2F4D8ECF70944523839255ECEC73C2AE","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"041F88DE61374B7C927382849E0F52CA","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 厅室数量"},{"cell_type":"code","metadata":{"_id":"86BF71B21EE74687877430C6F82A749E","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"201749888EB648618E8E364ABBB7BE44","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 处理房屋户型（室厅厨卫）\ndef extract_room_info(house_type):\n    \"\"\"提取房屋户型信息的函数\"\"\"\n    # 检查输入是否为字符串，非字符串的情况返回默认值\n    if not isinstance(house_type, str):\n        return {'室': 0, '厅': 0, '厨': 0, '卫': 0}\n    \n    # 使用正则表达式提取室、厅、厨、卫的数据\n    room_data = re.findall(r'(\\d+)室|(\\d+)厅|(\\d+)厨|(\\d+)卫', house_type)\n    \n    # 默认初始值为0\n    room_count = {'室': 0, '厅': 0, '厨': 0, '卫': 0}\n    \n    # 提取匹配的数据\n    for match in room_data:\n        for i, category in enumerate(['室', '厅', '厨', '卫']):\n            if match[i]:\n                room_count[category] = int(match[i])\n    \n    return room_count\n\ndef process_house_type(train_df, test_df, predict_data):\n    \"\"\"处理所有数据集的房屋户型\"\"\"\n    # 1. 填充缺失值为空字符串，避免正则表达式报错\n    for df in [train_df, test_df, predict_data]:\n        df['房屋户型'] = df['房屋户型'].fillna('')\n    \n    # 2. 提取户型信息到新列\n    for df in [train_df, test_df, predict_data]:\n        for room_type in ['室', '厅', '厨', '卫']:\n            df[room_type] = df['房屋户型'].apply(lambda x: extract_room_info(x)[room_type])\n    \n    # 3. 计算训练集的中位数，用于填充缺失值\n    room_medians = {\n        '室': train_df['室'].median(),\n        '厅': train_df['厅'].median()\n    }\n    \n    # 4. 使用中位数和常见值填充缺失值\n    for df in [train_df, test_df, predict_data]:\n        df['室'] = df['室'].fillna(room_medians['室'])\n        df['厅'] = df['厅'].fillna(room_medians['厅'])\n        df['厨'] = df['厨'].fillna(1)  # 使用常见值1填充\n        df['卫'] = df['卫'].fillna(1)  # 使用常见值1填充\n    \n    return train_df, test_df, predict_data\n\n# 处理所有数据集的房屋户型\ntrain_df, test_df, predict_data = process_house_type(train_df, test_df, predict_data)","outputs":[],"execution_count":25},{"cell_type":"markdown","metadata":{"_id":"E60F260453B64347A3C889CF51828A3E","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"677EF1DC562443A4941D4B98F23E369E","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"#  区域、板块"},{"cell_type":"code","metadata":{"_id":"D90B896D2D0344C8B9B66D02E8B01FD8","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E16AE3499CA64890B8BD63B7FF52D403","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"train_df['log_价格'] = np.log1p(train_df['价格'])\ntest_df['log_价格'] = np.log1p(test_df['价格'])\n# 对区域、板块进行 Target Encoding，区域板块类别多，使用独热编码维度过高\n#训练集：fit_transform，需要使用目标变量 log_价格\ntarget_enc = TargetEncoder(cols=['区域', '板块'])\ntrain_df[['区域', '板块']] = target_enc.fit_transform(train_df[['区域', '板块']], train_df['log_价格'])\n#验证集\ntest_df[['区域', '板块']] = target_enc.transform(test_df[['区域', '板块']])\n#测试集：直接 transform（测试集没有价格信息）\npredict_data[['区域', '板块']] = target_enc.transform(predict_data[['区域', '板块']])\n\n#对 Target Encoding 后的区域和板块进行归一化到 [-1,1]，有利于后续正则化\nminmax_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_df[['区域', '板块']] = minmax_scaler.fit_transform(train_df[['区域', '板块']])\ntest_df[['区域', '板块']] = minmax_scaler.transform(test_df[['区域', '板块']])\npredict_data[['区域', '板块']] = minmax_scaler.transform(predict_data[['区域', '板块']])","outputs":[],"execution_count":26},{"cell_type":"markdown","metadata":{"_id":"6F626A8002FD47A5AC9D4CB20A1EA368","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"6231B98D492D4EE3A89AD7033FFD88F6","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 自然语言处理"},{"cell_type":"markdown","metadata":{"_id":"9BFA25EDFE854044812AFE349603FE87","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"A340B3E2581244B289DEEAB88F0A8EC2","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"## 高频词"},{"cell_type":"code","metadata":{"_id":"BC677BF1546C4DDDA56BE732FB904BC7","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"D0EDB40C45D4443ABC8C362C6088565A","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"from collections import Counter","outputs":[],"execution_count":27},{"cell_type":"code","metadata":{"_id":"1728B9E079A8412CB84218962F5FCFA1","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"D53081026D6F4078A373026338E73FBA","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#因为核心卖点与户型介绍有很多重复的部分，所以合并起来\n#train_df['卖点-户型'] = train_df['核心卖点'] + train_df['户型介绍']\n#test_df['卖点-户型'] = test_df['核心卖点'] + test_df['户型介绍']\n#predict_data['卖点-户型'] = predict_data['核心卖点'] + predict_data['户型介绍']","outputs":[],"execution_count":28},{"cell_type":"code","metadata":{"_id":"3D611809F9E941F286195EC096232BD7","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"83B8B446CC2C4C9496BB7FC3E3A39C92","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#寻找高频词函数\ndef extract_high_frequency_words(df, column_name, top_n=10):\n    # 将指定列的空值替换为空字符串\n    df[column_name] = df[column_name].fillna('')\n\n    # 定义一个函数用于分词\n    def tokenize(text):\n        # 使用正则表达式匹配中文词语\n        words = re.findall(r'[\\u4e00-\\u9fa5]+', text)\n        return words\n\n    # 对指定列进行分词\n    all_words = []\n    for description in df[column_name]:\n        words = tokenize(description)\n        all_words.extend(words)\n\n    # 统计词频\n    word_counts = Counter(all_words)\n\n    # 显示前 top_n 个高频词\n    return word_counts.most_common(top_n)","outputs":[],"execution_count":29},{"cell_type":"code","metadata":{"_id":"14DAF39925AA4D86883E90B0BF7FCFB9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"0B23D169EFA54115B7E6577E07B66736","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"print(\"周边配套高频词:\",extract_high_frequency_words(train_df,'周边配套', top_n=33))\nprint(\"交通出行高频词:\",extract_high_frequency_words(train_df,'交通出行', top_n=20))\nprint(\"房屋优势高频词:\",extract_high_frequency_words(train_df,'房屋优势', top_n=20))","outputs":[{"output_type":"stream","name":"stdout","text":"周边配套高频词: [('米', 8489), ('医院', 5517), ('市', 4514), ('公里', 3612), ('建设银行', 3414), ('银行', 3400), ('工商银行', 3011), ('商场', 2737), ('路', 2591), ('公园', 2530), ('永辉', 2294), ('中国银行', 2189), ('农业银行', 2032), ('号线', 2011), ('华润万家', 1361), ('家乐福', 1353), ('沃尔玛', 1315), ('地铁', 1273), ('医疗', 1165), ('购物', 1165), ('配套齐全', 1018), ('交通银行', 1003), ('人人乐', 944), ('电影院', 904), ('小区', 898), ('出行方便', 881), ('生活便利', 838), ('物美', 824), ('距离', 809), ('交通便利', 776), ('距离小区', 764), ('万达广场', 738), ('招商银行', 737)]\n交通出行高频词: [('路', 76280), ('米', 12439), ('号线', 11583), ('地铁', 5562), ('公里', 4802), ('出行方便', 4236), ('公交', 3278), ('有', 3270), ('交通便利', 3165), ('轻轨', 2003), ('等', 1937), ('距离', 1726), ('路等', 1494), ('出行便利', 1490), ('米左右', 1352), ('公交线路', 1286), ('线', 1217), ('路公交车', 1177), ('距离地铁', 1078), ('公交站', 1000)]\n房屋优势高频词: [('装修', 29647), ('地铁', 26470), ('房本满五年', 25600), ('房本满两年', 13882)]\n"}],"execution_count":30},{"cell_type":"code","metadata":{"id":"0C3195062C1643BCB73296CFDF6BF646","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"pip install jieba","outputs":[{"output_type":"stream","name":"stdout","text":"Collecting jieba\n  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n     |████████████████████████████████| 19.2 MB 814 kB/s            07\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: jieba\n  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314476 sha256=bb49b6a353346199075088afc67be8a3c502a80fccf13b77bb6d6e7f5dcb16e3\n  Stored in directory: /home/mw/.cache/pip/wheels/24/aa/17/5bc7c72e9a37990a9620cc3aad0acad1564dcff6dbc2359de3\nSuccessfully built jieba\nInstalling collected packages: jieba\nSuccessfully installed jieba-0.42.1\nNote: you may need to restart the kernel to use updated packages.\n"}],"execution_count":31},{"cell_type":"code","metadata":{"id":"FBA43FC7A6774ABFB6555419C0C60DB1","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import re\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport jieba\nimport numpy as np\n\nclass RealEstateTextProcessor:\n    def __init__(self):\n        # 优化后的关键词字典 - 增加权重和同义词\n        self.keyword_categories = {\n            \"交通\": {\n                \"keywords\": [\"地铁\", \"公交\", \"轻轨\", \"主干道\", \"站点\", \"交通\", \"号线\", \"路\", \"米\", \"BRT\", \"快速路\", \"高架\"],\n                \"weight\": 1.2,  # 交通权重稍高\n                \"synonyms\": {\"地铁\": [\"轨道交通\", \"城铁\"], \"公交\": [\"巴士\", \"班车\"]}\n            },\n            \"配套\": {\n                \"keywords\": [\"医院\", \"诊所\", \"超市\", \"商场\", \"商圈\", \"购物\", \"百货\", \"广场\", \"公园\", \"银行\", \n                           \"图书馆\", \"绿地\", \"电影院\", \"家乐福\", \"永辉\", \"沃尔玛\", \"华润万家\", \"物美\", \"人人乐\"],\n                \"weight\": 1.0,\n                \"synonyms\": {\"医院\": [\"三甲\", \"医疗\"], \"超市\": [\"便利店\"]}\n            },\n            \"学区\": {\n                \"keywords\": [\"学校\", \"学区\", \"重点\", \"幼儿园\", \"小学\", \"中学\", \"大学\", \"名校\", \"示范\"],\n                \"weight\": 1.3,  # 学区房权重最高\n                \"synonyms\": {\"重点\": [\"名校\", \"示范\"], \"学区\": [\"教育\"]}\n            },\n            \"税费\": {\n                \"keywords\": [\"满五唯一\", \"税费\", \"免税\", \"满两年\", \"满五年\", \"个税\", \"契税\"],\n                \"weight\": 0.8,\n                \"synonyms\": {}\n            }\n        }\n        \n        # 初始化分词器\n        jieba.initialize()\n        \n    def smart_tokenize(self, text):\n        \"\"\"智能分词 - 结合正则和jieba\"\"\"\n        if pd.isna(text) or text == '':\n            return []\n        \n        # 先用jieba分词\n        jieba_words = list(jieba.cut(text))\n        # 再用正则提取中文词语\n        regex_words = re.findall(r'[\\u4e00-\\u9fa5]+', text)\n        \n        # 合并并去重\n        all_words = list(set(jieba_words + regex_words))\n        return [w for w in all_words if len(w) > 1]  # 过滤单字\n    \n    def extract_enhanced_keywords(self, text, category_info):\n        \"\"\"增强版关键词提取 - 支持同义词和权重\"\"\"\n        if pd.isna(text):\n            return 0\n        \n        keywords = category_info[\"keywords\"]\n        synonyms = category_info[\"synonyms\"]\n        weight = category_info[\"weight\"]\n        \n        score = 0\n        words = self.smart_tokenize(text)\n        \n        for word in words:\n            # 直接匹配\n            if word in keywords:\n                score += weight\n            # 同义词匹配\n            for key, syn_list in synonyms.items():\n                if word in syn_list:\n                    score += weight * 0.8  # 同义词权重稍低\n        \n        return min(score, 3)  # 限制最大得分，避免过度偏重\n    \n    def calculate_text_diversity(self, df, text_columns):\n        \"\"\"计算文本多样性得分\"\"\"\n        def diversity_score(row):\n            all_text = ' '.join([str(row[col]) for col in text_columns if pd.notna(row[col])])\n            words = self.smart_tokenize(all_text)\n            if len(words) == 0:\n                return 0\n            # 计算词汇丰富度\n            unique_words = len(set(words))\n            total_words = len(words)\n            return unique_words / total_words if total_words > 0 else 0\n        \n        return df.apply(diversity_score, axis=1)\n    \n    def extract_selling_points_tfidf(self, df, text_col='核心卖点', top_n=5):\n        \"\"\"使用TF-IDF提取核心卖点特征\"\"\"\n        # 清理文本\n        texts = df[text_col].fillna('').apply(lambda x: ' '.join(self.smart_tokenize(x)))\n        \n        if texts.str.len().sum() == 0:  # 如果没有文本内容\n            return pd.DataFrame()\n        \n        # TF-IDF向量化\n        vectorizer = TfidfVectorizer(\n            max_features=50,\n            ngram_range=(1, 2),  # 支持1-2元词组\n            min_df=2,\n            token_pattern=r'(?u)\\b\\w+\\b'\n        )\n        \n        try:\n            tfidf_matrix = vectorizer.fit_transform(texts)\n            feature_names = vectorizer.get_feature_names_out()\n            \n            # 为每个房源计算TF-IDF得分最高的特征\n            tfidf_scores = tfidf_matrix.toarray()\n            top_features = []\n            \n            for scores in tfidf_scores:\n                top_indices = scores.argsort()[-top_n:][::-1]\n                top_words = [feature_names[i] for i in top_indices if scores[i] > 0]\n                top_features.append('_'.join(top_words[:3]))  # 取前3个作为特征\n            \n            return pd.Series(top_features, index=df.index)\n        except:\n            return pd.Series([''] * len(df), index=df.index)\n    \n    def process_all_features(self, df, text_columns):\n        \"\"\"处理所有文本特征\"\"\"\n        df_processed = df.copy()\n        \n        # 1. 增强版关键词特征\n        for category, info in self.keyword_categories.items():\n            df_processed[f\"{category}_得分\"] = 0\n            for col in text_columns:\n                if col in df_processed.columns:\n                    scores = df_processed[col].apply(\n                        lambda x: self.extract_enhanced_keywords(x, info)\n                    )\n                    df_processed[f\"{category}_得分\"] += scores\n        \n        # 2. 文本多样性特征\n        df_processed[\"描述丰富度\"] = self.calculate_text_diversity(df_processed, text_columns)\n        \n        # 3. TF-IDF核心卖点特征\n        if '核心卖点' in df_processed.columns:\n            df_processed[\"核心卖点_特征\"] = self.extract_selling_points_tfidf(df_processed)\n        \n        # 4. 文本长度特征\n        for col in text_columns:\n            if col in df_processed.columns:\n                df_processed[f\"{col}_长度\"] = df_processed[col].fillna('').str.len()\n        \n        # 5. 综合评分\n        score_cols = [f\"{cat}_得分\" for cat in self.keyword_categories.keys()]\n        df_processed[\"综合文本得分\"] = df_processed[score_cols].sum(axis=1) + df_processed[\"描述丰富度\"] * 2\n        \n        return df_processed\n\n# 使用示例\ndef process_datasets(train_df, test_df, predict_data):\n    \"\"\"处理所有数据集\"\"\"\n    processor = RealEstateTextProcessor()\n    text_columns = [\"房屋优势\", \"核心卖点\", \"周边配套\", \"交通出行\"]\n    \n    # 处理所有数据集\n    train_df = processor.process_all_features(train_df, text_columns)\n    test_df = processor.process_all_features(test_df, text_columns)\n    predict_data = processor.process_all_features(predict_data, text_columns)\n    \n    # 按小区填补缺失的得分特征\n    score_columns = [f\"{cat}_得分\" for cat in processor.keyword_categories.keys()]\n    \n    for df in [train_df, test_df, predict_data]:\n        for col in score_columns:\n            df[col] = df.groupby(\"小区名称\")[col].transform(\n                lambda x: x.fillna(x.mean()) if x.mean() > 0 else x.fillna(0)\n            )\n    \n    return train_df, test_df, predict_data\n\n# 调用处理函数\ntrain_df, test_df, predict_data = process_datasets(train_df, test_df, predict_data)","outputs":[{"output_type":"stream","name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.840 seconds.\nPrefix dict has been built successfully.\n"}],"execution_count":32},{"cell_type":"code","metadata":{"id":"2C372A9AC0D84E94984815AE675CA74C","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef preprocess_for_xgboost(train_df, test_df, predict_data):\n    \"\"\"\n    为XGBoost优化特征预处理\n    \"\"\"\n    # 复制数据避免修改原始数据\n    train_processed = train_df.copy()\n    test_processed = test_df.copy() \n    predict_processed = predict_data.copy()\n    \n    # 1. 文本长度特征 - Log变换处理极端值\n    length_cols = [col for col in train_processed.columns if col.endswith('_长度')]\n    \n    for col in length_cols:\n        for df in [train_processed, test_processed, predict_processed]:\n            # Log(x+1)变换，避免log(0)\n            df[f\"{col}_log\"] = np.log1p(df[col])\n            # 可选：保留原特征或删除\n            # df.drop(col, axis=1, inplace=True)\n    \n    # 2. 综合文本得分 - RobustScaler（基于训练集拟合）\n    if '综合文本得分' in train_processed.columns:\n        scaler = RobustScaler()\n        \n        # 只用训练集拟合\n        train_score = train_processed[['综合文本得分']].values\n        scaler.fit(train_score)\n        \n        # 应用到所有数据集\n        train_processed['综合文本得分_scaled'] = scaler.transform(train_score).flatten()\n        test_processed['综合文本得分_scaled'] = scaler.transform(\n            test_processed[['综合文本得分']].values).flatten()\n        predict_processed['综合文本得分_scaled'] = scaler.transform(\n            predict_processed[['综合文本得分']].values).flatten()\n    \n    # 3. 描述丰富度 - 通常不需要处理，但可以创建分箱特征\n    if '描述丰富度' in train_processed.columns:\n        # 创建分箱特征（可选）\n        bins = [0, 0.3, 0.6, 0.8, 1.0]\n        labels = ['低', '中低', '中高', '高']\n        \n        for df in [train_processed, test_processed, predict_processed]:\n            df['描述丰富度_分档'] = pd.cut(df['描述丰富度'], bins=bins, labels=labels, include_lowest=True)\n            # 转换为数值型\n            df['描述丰富度_分档'] = df['描述丰富度_分档'].cat.codes\n    \n    # 4. 处理核心卖点特征（字符串转数值）\n    if '核心卖点_特征' in train_processed.columns:\n        # 收集所有唯一值\n        all_unique = set()\n        for df in [train_processed, test_processed, predict_processed]:\n            all_unique.update(df['核心卖点_特征'].unique())\n        \n        # 创建映射字典\n        feature_map = {feature: idx for idx, feature in enumerate(sorted(all_unique))}\n        \n        # 应用映射\n        for df in [train_processed, test_processed, predict_processed]:\n            df['核心卖点_特征_encoded'] = df['核心卖点_特征'].map(feature_map).fillna(-1)\n    \n    # 5. 创建特征交互（XGBoost会自动学习，但手动创建几个有用的）\n    interaction_features = []\n    \n    for df in [train_processed, test_processed, predict_processed]:\n        # 学区 × 交通（学区房的交通便利性）\n        if '学区_得分' in df.columns and '交通_得分' in df.columns:\n            df['学区交通_交互'] = df['学区_得分'] * df['交通_得分']\n            interaction_features.append('学区交通_交互')\n        \n        # 配套 × 描述丰富度（配套好且描述详细）\n        if '配套_得分' in df.columns and '描述丰富度' in df.columns:\n            df['配套描述_交互'] = df['配套_得分'] * df['描述丰富度']\n            interaction_features.append('配套描述_交互')\n    \n    print(\"✅ XGBoost预处理完成！\")\n    print(f\"📊 新增特征: {len(length_cols)} 个Log变换 + 综合得分标准化 + 特征交互\")\n    \n    return train_processed, test_processed, predict_processed\n\n# 6. XGBoost特征重要性分析辅助函数\ndef analyze_text_features(model, feature_names):\n    \"\"\"\n    分析文本特征在XGBoost中的重要性\n    \"\"\"\n    if hasattr(model, 'feature_importances_'):\n        importance_df = pd.DataFrame({\n            'feature': feature_names,\n            'importance': model.feature_importances_\n        }).sort_values('importance', ascending=False)\n        \n        # 筛选文本相关特征\n        text_features = importance_df[\n            importance_df['feature'].str.contains('得分|长度|丰富度|卖点|交互')\n        ].head(10)\n        \n        print(\"🔥 文本特征重要性 TOP10:\")\n        for _, row in text_features.iterrows():\n            print(f\"   {row['feature']}: {row['importance']:.4f}\")\n        \n        return text_features\n    \n    return None\n\n# 使用示例\ntrain_df, test_df, predict_data = preprocess_for_xgboost(train_df, test_df, predict_data)","outputs":[{"output_type":"stream","name":"stdout","text":"✅ XGBoost预处理完成！\n📊 新增特征: 4 个Log变换 + 综合得分标准化 + 特征交互\n"}],"execution_count":33},{"cell_type":"code","metadata":{"id":"96A2993BD6DE44298F1A24949678D05E","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"print(\"当前所有列名:\", train_df.columns.tolist())","outputs":[{"output_type":"stream","name":"stdout","text":"当前所有列名: ['城市', '区域', '板块', '环线', '小区名称', '价格', '房屋户型', '所在楼层', '建筑面积', '梯户比例', '配备电梯', '房屋优势', '核心卖点', '户型介绍', '周边配套', '交通出行', 'lon', 'lat', '匹配小区', '建筑年代', '房屋总数', '楼栋总数', '绿 化 率', '容 积 率', '物 业 费', '停车位', '停车费用', '平均每平米月租金', '交易年份', '建筑结构编码_混合结构', '建筑结构编码_钢混结构', '装修编码_毛坯', '装修编码_简装', '装修编码_精装', '房屋数量', '楼栋数量', '绿化率', '容积率', '物业费', '停车位数量', '停车费', 'poly_房屋数量', 'poly_楼栋数量', 'poly_绿化率', 'poly_容积率', 'poly_物业费', 'poly_停车位数量', 'poly_停车费', 'poly_房屋数量^2', 'poly_房屋数量 楼栋数量', 'poly_房屋数量 绿化率', 'poly_房屋数量 容积率', 'poly_房屋数量 物业费', 'poly_房屋数量 停车位数量', 'poly_房屋数量 停车费', 'poly_楼栋数量^2', 'poly_楼栋数量 绿化率', 'poly_楼栋数量 容积率', 'poly_楼栋数量 物业费', 'poly_楼栋数量 停车位数量', 'poly_楼栋数量 停车费', 'poly_绿化率^2', 'poly_绿化率 容积率', 'poly_绿化率 物业费', 'poly_绿化率 停车位数量', 'poly_绿化率 停车费', 'poly_容积率^2', 'poly_容积率 物业费', 'poly_容积率 停车位数量', 'poly_容积率 停车费', 'poly_物业费^2', 'poly_物业费 停车位数量', 'poly_物业费 停车费', 'poly_停车位数量^2', 'poly_停车位数量 停车费', 'poly_停车费^2', 'area_scaled', 'area_squared', '主要朝向_北', '主要朝向_南', '主要朝向_西', '环线_填补', '城市_环线_0_二环内', '城市_环线_0_二至三环', '城市_环线_0_五至六环', '城市_环线_0_六环外', '城市_环线_0_四至五环', '城市_环线_0_无', '城市_环线_1_无环线', '城市_环线_2_内环内', '城市_环线_2_内环至外环', '城市_环线_2_外环外', '城市_环线_2_无', '城市_环线_3_中环至外环', '城市_环线_3_内环内', '城市_环线_3_内环至中环', '城市_环线_3_外环外', '城市_环线_3_无', '城市_环线_4_一环内', '城市_环线_4_一至二环', '城市_环线_4_三至四环', '城市_环线_4_二至三环', '城市_环线_4_四环外', '城市_环线_4_无', '城市_环线_5_无环线', '城市_环线_6_一环内', '城市_环线_6_一至二环', '城市_环线_6_三环外', '城市_环线_6_二至三环', '城市_环线_6_无', '城市_环线_0_二环内_建筑面积', '城市_环线_0_二至三环_建筑面积', '城市_环线_0_五至六环_建筑面积', '城市_环线_0_六环外_建筑面积', '城市_环线_0_四至五环_建筑面积', '城市_环线_0_无_建筑面积', '城市_环线_1_无环线_建筑面积', '城市_环线_2_内环内_建筑面积', '城市_环线_2_内环至外环_建筑面积', '城市_环线_2_外环外_建筑面积', '城市_环线_2_无_建筑面积', '城市_环线_3_中环至外环_建筑面积', '城市_环线_3_内环内_建筑面积', '城市_环线_3_内环至中环_建筑面积', '城市_环线_3_外环外_建筑面积', '城市_环线_3_无_建筑面积', '城市_环线_4_一环内_建筑面积', '城市_环线_4_一至二环_建筑面积', '城市_环线_4_三至四环_建筑面积', '城市_环线_4_二至三环_建筑面积', '城市_环线_4_四环外_建筑面积', '城市_环线_4_无_建筑面积', '城市_环线_5_无环线_建筑面积', '城市_环线_6_一环内_建筑面积', '城市_环线_6_一至二环_建筑面积', '城市_环线_6_三环外_建筑面积', '城市_环线_6_二至三环_建筑面积', '城市_环线_6_无_建筑面积', '户梯比', '室', '厅', '厨', '卫', 'log_价格', '交通_得分', '配套_得分', '学区_得分', '税费_得分', '描述丰富度', '核心卖点_特征', '房屋优势_长度', '核心卖点_长度', '周边配套_长度', '交通出行_长度', '综合文本得分', '房屋优势_长度_log', '核心卖点_长度_log', '周边配套_长度_log', '交通出行_长度_log', '综合文本得分_scaled', '描述丰富度_分档', '核心卖点_特征_encoded', '学区交通_交互', '配套描述_交互']\n"}],"execution_count":34},{"cell_type":"code","metadata":{"_id":"0DA8B3D5B5FB4FC1A3535CEB4880A39A","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CC7C8105302F496AAC44B89C28DD9FC5","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 关键词字典\nkeywords = {\n    \"交通\": [\"地铁\", \"公交\", \"轻轨\",\"主干道\",\"站点\",\"交通\",\"号线\",\"路\",\"米\"],\n    \"配套\": [\"医院\", \"诊所\",\"诊所\",\"超市\", \"商场\", \"商圈\",\"购物\", \"百货\", \"广场\", \"公园\", \"银行\", \"图书馆\",\"绿地\",\"电影院\",\n             \"家乐福\",\"永辉\",\"沃尔玛\",\"华润万家\",\"物美\",\"人人乐\"],\n    \"学区\": [\"学校\", \"学区\",\"重点\",\"幼儿园\",\"小学\",\"中学\",\"大学\"],\n    \"税费\": [\"满五唯一\",\"税费\",\"免税\",\"满两年\",\"满五年\"],\n}\n\n# 处理文本特征的函数\ndef extract_keywords(text, keyword_list):\n    if pd.isna(text):\n        return 0  # 缺失值填充为0\n    for word in keyword_list:\n        if word in text:\n            return 1  # 只要出现关键词，就记为1\n    return 0\n\n# 需要处理的字段\ntext_columns = [\"房屋优势\", \"核心卖点\", \"周边配套\", \"交通出行\"]\n\n#处理所有数据集\ndef process_text_features(df):\n    for category, words in keywords.items():\n        df[f\"{category}_关键词\"] = 0\n        for col in text_columns:\n            df[f\"{category}_关键词\"] |= df[col].apply(lambda x: extract_keywords(x, words))\n    # 处理户型介绍变量\n    df[\"户型介绍_是否填写\"] = df[\"户型介绍\"].notna().astype(int)\n    return df\n\n# 处理所有数据集\ntrain_df = process_text_features(train_df)\ntest_df = process_text_features(test_df)\npredict_data = process_text_features(predict_data)\n\n# 按小区填补缺失值（针对“交通_关键词”和“配套_关键词”）\ndef fill_missing_by_group(df, group_col, fill_cols):\n    for col in fill_cols:\n        df[col] = df.groupby(group_col)[col].transform(lambda x: x.fillna(x.max()))\n    return df\n\nfill_columns = [\"交通_关键词\", \"配套_关键词\",\"学区_关键词\"]\n\n# 对所有数据集按小区填补\ntrain_df = fill_missing_by_group(train_df, \"小区名称\", fill_columns)\ntest_df = fill_missing_by_group(test_df, \"小区名称\", fill_columns)\npredict_data = fill_missing_by_group(predict_data, \"小区名称\", fill_columns)\n\n# 计算匹配的关键词类别数量\ndef count_keyword_categories(df):\n    df[\"关键词匹配数量\"] = (\n        df[\"交通_关键词\"] + \n        df[\"配套_关键词\"] + \n        df[\"税费_关键词\"] +\n        df[\"户型介绍_是否填写\"]\n    )\n    return df\n\n# 处理所有数据集\ntrain_df = count_keyword_categories(train_df)\ntest_df = count_keyword_categories(test_df)\npredict_data = count_keyword_categories(predict_data)","outputs":[],"execution_count":35},{"cell_type":"code","metadata":{"id":"DC47ABA1C53A40789E5841F1BC78A294","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"print(\"train_df 类型:\", type(train_df))","outputs":[{"output_type":"stream","name":"stdout","text":"train_df 类型: <class 'pandas.core.frame.DataFrame'>\n"}],"execution_count":36},{"cell_type":"markdown","metadata":{"_id":"9138A6F7B6D94FB59ACB515A6BF3415A","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"DFE13F588CAC4DFBB6F4163CCD59EDEB","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 楼层处理"},{"cell_type":"code","metadata":{"_id":"FC78C99E8D1046B48492539DCB6D357A","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"9E0B6A2A38D54F949EC980BB29B26C1A","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"def extract_floor_info(s):\n    try:\n        \n        # 优先检查是否包含地下室\n        if \"地下室\" in s:\n            # 地下室逻辑：总楼层设为1，当前楼层设为0或-1（根据业务需求）\n            return -1, 1  # 当前楼层=0（表示地下室），总楼层=1\n        total = re.search(r'共(\\d+)层', s).group(1)\n        level = re.search(r'(\\d+)层', s.split('(')[0])\n\n        if level:\n            current = level.group(1)\n        else:\n            desc = s.split('(')[0].strip()\n            total = int(total)\n            if '低' in desc:\n                current = total * 0.25\n            elif '中' in desc:\n                current = total * 0.5\n            elif '高' in desc:\n                current = total * 0.75\n            elif '底' in desc:\n                current = 1\n            else:\n                current = total * 1\n        return int(float(current)), int(total)\n    except:\n        return np.nan, np.nan\n\nfor df in [train_df,test_df,predict_data]:\n    df[['当前楼层', '总楼层']] = df['所在楼层'].apply(\n        lambda x: pd.Series(extract_floor_info(x) if isinstance(x, str) else (np.nan, np.nan)))\n\n# 检查 数据集 里是否有 '总楼层'\nprint(train_df.columns)  \nprint(predict_data.columns)\n\ndef add_floor_features(*dfs):\n    \"\"\"\n    批量添加楼层相关特征到多个DataFrame（直接修改原DataFrame）\n    参数:\n        *dfs: 一个或多个包含\"总楼层\"和\"当前楼层\"的DataFrame\n    \"\"\"\n    for df in dfs:\n        # 1. 基本特征\n        df[\"多层住宅\"] = (df[\"总楼层\"] <= 6).astype(int)\n        \n        # 2. 黄金楼层特征\n        df[\"黄金楼层\"] = (\n            ((df[\"多层住宅\"] == 1) & (df[\"当前楼层\"].between(2, 4))) |\n            ((df[\"多层住宅\"] == 0) & (df[\"当前楼层\"] >= (df[\"总楼层\"] * 0.5)))\n        ).astype(int)\n        \n        # 3. 楼层占比特征\n        df[\"楼层占比\"] = df[\"当前楼层\"] / df[\"总楼层\"]\n        df[\"楼层占比_平方\"] = df[\"楼层占比\"] ** 2\n        \n        # 4. 交互特征\n        df[\"楼层_电梯交互\"] = df[\"楼层占比\"] * df[\"多层住宅\"]\n        df[\"楼层_高层交互\"] = df[\"楼层占比\"] * (1 - df[\"多层住宅\"])\n\n# 处理所有数据集\nadd_floor_features(train_df, test_df, predict_data)","outputs":[{"output_type":"stream","name":"stdout","text":"Index(['城市', '区域', '板块', '环线', '小区名称', '价格', '房屋户型', '所在楼层', '建筑面积', '梯户比例',\n       ...\n       '学区交通_交互', '配套描述_交互', '交通_关键词', '配套_关键词', '学区_关键词', '税费_关键词',\n       '户型介绍_是否填写', '关键词匹配数量', '当前楼层', '总楼层'],\n      dtype='object', length=172)\nIndex(['城市', '区域', '板块', '环线', '小区名称', '房屋户型', '所在楼层', '建筑面积', '梯户比例', '配备电梯',\n       ...\n       '学区交通_交互', '配套描述_交互', '交通_关键词', '配套_关键词', '学区_关键词', '税费_关键词',\n       '户型介绍_是否填写', '关键词匹配数量', '当前楼层', '总楼层'],\n      dtype='object', length=171)\n"}],"execution_count":37},{"cell_type":"code","metadata":{"_id":"DB3E136045FF4531A058A09CE3E764C8","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"BD41244E50BE46FCABBAAFBD8CD4AA12","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"def extract_floor_info(s):\n    try:\n        if not isinstance(s, str):\n            return np.nan, np.nan\n            \n        # 处理地下室情况\n        if \"地下室\" in s:\n            return -1, 1\n            \n        # 匹配总楼层：从括号内提取\"共XX层\"\n        total_match = re.search(r'共(\\d+)层', s)\n        if not total_match:\n            return np.nan, np.nan\n        total = int(total_match.group(1))\n        \n        # 匹配当前楼层描述（括号前部分）\n        desc = s.split('(')[0].strip()\n        \n        # 处理明确楼层数字的情况（如\"3层 (共26层)\"）\n        level_match = re.search(r'(\\d+)层', desc)\n        if level_match:\n            return int(level_match.group(1)), total\n            \n        # 处理文字描述（低/中/高/底层）\n        if '低' in desc:\n            if total <= 2:  # 总楼层为1或2时，当前楼层取1\n                current = 1\n            else:\n                current = round(total * 0.25)\n        elif '中' in desc:\n            current = round(total * 0.5)\n        elif '高' in desc:\n            current = round(total * 0.75)\n        elif '底' in desc:\n            current = 1\n        else:  # 顶层\n            current = total\n            \n        return min(int(current), total), total  # 确保不超过总楼层\n    except:\n        return np.nan, np.nan\n\nfor df in [train_df,test_df,predict_data]:\n    df[['当前楼层', '总楼层']] = df['所在楼层'].apply(\n        lambda x: pd.Series(extract_floor_info(x) if isinstance(x, str) else (np.nan, np.nan)))","outputs":[],"execution_count":38},{"cell_type":"code","metadata":{"_id":"686917B0DADD4BEBA0911D036FFF8EAF","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"68848C5AE2C94542BB716528D92092AE","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 检查训练集\nprint(\"训练集中当前楼层为0的记录数量:\", train_df[train_df[\"当前楼层\"] == 0].shape[0])\n\nnan_count = train_df['当前楼层'].isna().sum()\nprint(\"当前楼层中NaN值的个数:\", nan_count)","outputs":[{"output_type":"stream","name":"stdout","text":"训练集中当前楼层为0的记录数量: 0\n当前楼层中NaN值的个数: 0\n"}],"execution_count":39},{"cell_type":"code","metadata":{"_id":"DD6EB35BEF5C4E24A00EC05EAA5E69FD","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"A57618F9CCC943C9B11883DC27F5553B","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"def add_floor_features(*dfs):\n    \"\"\"\n    批量添加楼层相关特征到多个DataFrame（直接修改原DataFrame）\n    参数:\n        *dfs: 一个或多个包含\"总楼层\"和\"当前楼层\"的DataFrame\n    \"\"\"\n    for df in dfs:\n        # 1. 基本特征\n        df[\"多层住宅\"] = (df[\"总楼层\"] <= 6).astype(int)\n        \n        # 2. 黄金楼层特征\n        df[\"黄金楼层\"] = (\n            ((df[\"多层住宅\"] == 1) & (df[\"当前楼层\"].between(2, 4))) |\n            ((df[\"多层住宅\"] == 0) & (df[\"当前楼层\"] >= (df[\"总楼层\"] * 0.5)))\n        ).astype(int)\n        \n        # 3. 楼层占比特征\n        df[\"楼层占比\"] = df[\"当前楼层\"] / df[\"总楼层\"]\n        df[\"楼层占比_平方\"] = df[\"楼层占比\"] ** 2\n        \n        # 4. 交互特征\n        df[\"楼层_电梯交互\"] = df[\"楼层占比\"] * df[\"多层住宅\"]\n        df[\"楼层_高层交互\"] = df[\"楼层占比\"] * (1 - df[\"多层住宅\"])\n\n# 使用示例：一次性处理所有数据集\nadd_floor_features(train_df, test_df, predict_data)","outputs":[],"execution_count":40},{"cell_type":"code","metadata":{"_id":"E7037D08CDA949ED85C43611F73C23CC","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"7640D0384FD8464984B418769CACD9D7","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 检查训练集\nprint(\"训练集中楼层占比为0的记录数量:\", train_df[train_df[\"楼层占比\"] == 0].shape[0])\n\nnan_count = train_df['楼层占比'].isna().sum()\nprint(\"楼层占比中NaN值的个数:\", nan_count)","outputs":[{"output_type":"stream","name":"stdout","text":"训练集中楼层占比为0的记录数量: 0\n楼层占比中NaN值的个数: 0\n"}],"execution_count":41},{"cell_type":"markdown","metadata":{"_id":"F547B502F47E466DAE7E6A880E84C8EF","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E0D70BAA947B4ED28C230B16DCE9148D","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 房龄处理"},{"cell_type":"code","metadata":{"id":"E919D6D291D04388A80DB9FCD80E75F9","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"for df in [train_df, test_df, predict_data]:\n    df['房龄'] = df['交易年份'] - df['建筑年代']\n\n# 检查房龄范围\nprint(train_df['房龄'].describe())","outputs":[{"output_type":"stream","name":"stdout","text":"count    68450.000000\nmean        17.092403\nstd         10.290071\nmin          1.000000\n25%          9.000000\n50%         14.000000\n75%         23.000000\nmax         69.000000\nName: 房龄, dtype: float64\n"}],"execution_count":42},{"cell_type":"code","metadata":{"_id":"C3F35413D8EB4AB1A446D05B6BF2707D","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"5D05B9F222CA408E8AA0D8D7DEC7058D","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"print(train_df.groupby(\"多层住宅\")[\"房龄\"].describe())\n# 计算相关性矩阵\ncorr_matrix = train_df[['房龄','多层住宅','建筑面积',\"总楼层\",\"容 积 率\"]].corr()\nprint(corr_matrix['房龄'])  # 观察房龄与面积、总楼层的相关性\nimport seaborn as sns\n# 绘制分组箱线图\nimport matplotlib.pyplot as plt\n# 设置图形风格\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(8, 6))\n\n# 绘制箱线图\nax = sns.boxplot(x=\"多层住宅\", y=\"房龄\", data=train_df, palette=\"Set2\")\n\n# 添加标题和坐标轴标签\nax.set_title(\"Boxplot of Building Age by Multi-Story Residential\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"Multi-Story Residential\", fontsize=12)\nax.set_ylabel(\"Building Age\", fontsize=12)\n\n# 调整刻度字体大小\nax.tick_params(axis='both', labelsize=10)\n\n# 显示图形\nplt.show()\n# 检查缺失比例\nmissing_rate_by_group = train_df.groupby(\"多层住宅\")[\"房龄\"].apply(lambda x: x.isna().mean())\nprint(missing_rate_by_group)","outputs":[{"output_type":"stream","name":"stdout","text":"        count       mean        std  min   25%   50%   75%   max\n多层住宅                                                            \n0     55513.0  15.462811   9.257333  1.0   9.0  13.0  20.0  54.0\n1     12937.0  24.085027  11.498987  1.0  15.0  23.0  33.0  69.0\n房龄       1.000000\n多层住宅     0.328053\n建筑面积    -0.176634\n总楼层     -0.414447\n容 积 率   -0.129483\nName: 房龄, dtype: float64\n"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x432 with 1 Axes>","text/html":"<img src=\"/s3/static-files/upload/rt/4FAD53CBBF5D482EAE3F18ED5087E610/sx6iirn2u5.png\">"},"metadata":{}},{"output_type":"stream","name":"stdout","text":"多层住宅\n0    0.045020\n1    0.029337\nName: 房龄, dtype: float64\n"}],"execution_count":43},{"cell_type":"code","metadata":{"_id":"A57889FAEEA24380ADC6E2ADBF301340","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"53ACBEA1F5F84C9D9BC557A60D9FCC9A","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"#1. 在训练集上计算各组的房龄中位数\nmedian_age_multilayer = train_df[train_df[\"多层住宅\"] == 1][\"房龄\"].median()  # 多层住宅（1）的中位数\nmedian_age_non_multilayer = train_df[train_df[\"多层住宅\"] == 0][\"房龄\"].median()  # 非多层住宅（0）的中位数\n\n# 2. 定义填充函数\ndef fill_age_by_building_type(df, median_multilayer, median_non_multilayer):\n    # 处理极端值\n    df.loc[df[\"房龄\"] > 100, \"房龄\"] = 100\n    df.loc[df[\"房龄\"] < 0, \"房龄\"] = np.nan\n    \n    # 分组填充缺失值\n    df[\"房龄\"] = df.apply(\n        lambda row: median_multilayer if (pd.isna(row[\"房龄\"]) and row[\"多层住宅\"] == 1)\n                   else (median_non_multilayer if (pd.isna(row[\"房龄\"]) and row[\"多层住宅\"] == 0)\n                   else row[\"房龄\"]),\n        axis=1\n    )\n    return df\n\n# 3. 应用填充\ntrain_df = fill_age_by_building_type(train_df, median_age_multilayer, median_age_non_multilayer)\ntest_df = fill_age_by_building_type(test_df, median_age_multilayer, median_age_non_multilayer)\npredict_data = fill_age_by_building_type(predict_data, median_age_multilayer, median_age_non_multilayer)\n\n #检查房龄列中的NaN数量\nnan_count = train_df['房龄'].isna().sum()\nprint(\"房龄中NaN值的个数:\", nan_count)","outputs":[{"output_type":"stream","name":"stdout","text":"房龄中NaN值的个数: 0\n"}],"execution_count":44},{"cell_type":"code","metadata":{"_id":"626BE13C396E482089847894798DEB11","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"79AD28DA1F58476881F7EC781AB05B0A","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"train_df['面积_房龄'] = train_df['area_scaled'] * train_df['房龄']\ntest_df['面积_房龄'] = test_df['area_scaled'] * test_df['房龄']\npredict_data['面积_房龄']=predict_data['area_scaled'] * predict_data['房龄']\n\n# 对房龄做非线性转换\ntrain_df['房龄_平方'] = train_df['房龄'] ** 2\ntest_df['房龄_平方'] = test_df['房龄'] ** 2\npredict_data['房龄_平方'] = predict_data['房龄'] ** 2\n\n# 对新特征进行标准化\nnew_features = ['面积_房龄', '房龄_平方']\nscaler_new = StandardScaler()\ntrain_df[new_features] = scaler_new.fit_transform(train_df[new_features])\ntest_df[new_features] = scaler_new.transform(test_df[new_features])\npredict_data[new_features] = scaler_new.transform(predict_data[new_features])\n\n# 需要标准化的特征\nstandard_features = ['房龄', '楼层占比','楼层占比_平方', '楼层_电梯交互', '楼层_高层交互']\n\n# 对连续特征进行标准化\nscaler = StandardScaler()\ntrain_df[standard_features] = scaler.fit_transform(train_df[standard_features])\ntest_df[standard_features] = scaler.transform(test_df[standard_features])\npredict_data[standard_features] = scaler.transform(predict_data[standard_features])","outputs":[],"execution_count":45},{"cell_type":"code","metadata":{"id":"952AF9044A894275877E985784B0EAA9","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"train_df['单位租金_平方'] = train_df['平均每平米月租金'] ** 2\ntest_df['单位租金_平方'] = test_df['平均每平米月租金'] ** 2\npredict_data['单位租金_平方'] = predict_data['平均每平米月租金'] ** 2\n\ntrain_df['卧室_租金'] = train_df['室'] * train_df['平均每平米月租金']\ntest_df['卧室_租金'] = test_df['室'] * test_df['平均每平米月租金']\npredict_data['卧室_租金'] = predict_data['室'] * predict_data['平均每平米月租金']\n\ntrain_df['面积_租金'] = train_df['area_scaled'] * train_df['平均每平米月租金']\ntest_df['面积_租金'] = test_df['area_scaled'] * test_df['平均每平米月租金']\npredict_data['面积_租金'] = predict_data['area_scaled'] * predict_data['平均每平米月租金']\n\nnewnew_features = ['单位租金_平方', '卧室_租金','面积_租金']\nscaler_new = StandardScaler()\ntrain_df[newnew_features] = scaler_new.fit_transform(train_df[newnew_features])\ntest_df[newnew_features] = scaler_new.transform(test_df[newnew_features])\npredict_data[newnew_features] = scaler_new.transform(predict_data[newnew_features])","outputs":[],"execution_count":46},{"cell_type":"code","metadata":{"_id":"6C8737ECE8614136878C4F79E339EDE4","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"348AD92FF5404E5585CCE7B8BB79CFFE","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 检查数据集中的列\nprint(\"当前所有列名:\", train_df.columns.tolist())","outputs":[{"output_type":"stream","name":"stdout","text":"当前所有列名: ['城市', '区域', '板块', '环线', '小区名称', '价格', '房屋户型', '所在楼层', '建筑面积', '梯户比例', '配备电梯', '房屋优势', '核心卖点', '户型介绍', '周边配套', '交通出行', 'lon', 'lat', '匹配小区', '建筑年代', '房屋总数', '楼栋总数', '绿 化 率', '容 积 率', '物 业 费', '停车位', '停车费用', '平均每平米月租金', '交易年份', '建筑结构编码_混合结构', '建筑结构编码_钢混结构', '装修编码_毛坯', '装修编码_简装', '装修编码_精装', '房屋数量', '楼栋数量', '绿化率', '容积率', '物业费', '停车位数量', '停车费', 'poly_房屋数量', 'poly_楼栋数量', 'poly_绿化率', 'poly_容积率', 'poly_物业费', 'poly_停车位数量', 'poly_停车费', 'poly_房屋数量^2', 'poly_房屋数量 楼栋数量', 'poly_房屋数量 绿化率', 'poly_房屋数量 容积率', 'poly_房屋数量 物业费', 'poly_房屋数量 停车位数量', 'poly_房屋数量 停车费', 'poly_楼栋数量^2', 'poly_楼栋数量 绿化率', 'poly_楼栋数量 容积率', 'poly_楼栋数量 物业费', 'poly_楼栋数量 停车位数量', 'poly_楼栋数量 停车费', 'poly_绿化率^2', 'poly_绿化率 容积率', 'poly_绿化率 物业费', 'poly_绿化率 停车位数量', 'poly_绿化率 停车费', 'poly_容积率^2', 'poly_容积率 物业费', 'poly_容积率 停车位数量', 'poly_容积率 停车费', 'poly_物业费^2', 'poly_物业费 停车位数量', 'poly_物业费 停车费', 'poly_停车位数量^2', 'poly_停车位数量 停车费', 'poly_停车费^2', 'area_scaled', 'area_squared', '主要朝向_北', '主要朝向_南', '主要朝向_西', '环线_填补', '城市_环线_0_二环内', '城市_环线_0_二至三环', '城市_环线_0_五至六环', '城市_环线_0_六环外', '城市_环线_0_四至五环', '城市_环线_0_无', '城市_环线_1_无环线', '城市_环线_2_内环内', '城市_环线_2_内环至外环', '城市_环线_2_外环外', '城市_环线_2_无', '城市_环线_3_中环至外环', '城市_环线_3_内环内', '城市_环线_3_内环至中环', '城市_环线_3_外环外', '城市_环线_3_无', '城市_环线_4_一环内', '城市_环线_4_一至二环', '城市_环线_4_三至四环', '城市_环线_4_二至三环', '城市_环线_4_四环外', '城市_环线_4_无', '城市_环线_5_无环线', '城市_环线_6_一环内', '城市_环线_6_一至二环', '城市_环线_6_三环外', '城市_环线_6_二至三环', '城市_环线_6_无', '城市_环线_0_二环内_建筑面积', '城市_环线_0_二至三环_建筑面积', '城市_环线_0_五至六环_建筑面积', '城市_环线_0_六环外_建筑面积', '城市_环线_0_四至五环_建筑面积', '城市_环线_0_无_建筑面积', '城市_环线_1_无环线_建筑面积', '城市_环线_2_内环内_建筑面积', '城市_环线_2_内环至外环_建筑面积', '城市_环线_2_外环外_建筑面积', '城市_环线_2_无_建筑面积', '城市_环线_3_中环至外环_建筑面积', '城市_环线_3_内环内_建筑面积', '城市_环线_3_内环至中环_建筑面积', '城市_环线_3_外环外_建筑面积', '城市_环线_3_无_建筑面积', '城市_环线_4_一环内_建筑面积', '城市_环线_4_一至二环_建筑面积', '城市_环线_4_三至四环_建筑面积', '城市_环线_4_二至三环_建筑面积', '城市_环线_4_四环外_建筑面积', '城市_环线_4_无_建筑面积', '城市_环线_5_无环线_建筑面积', '城市_环线_6_一环内_建筑面积', '城市_环线_6_一至二环_建筑面积', '城市_环线_6_三环外_建筑面积', '城市_环线_6_二至三环_建筑面积', '城市_环线_6_无_建筑面积', '户梯比', '室', '厅', '厨', '卫', 'log_价格', '交通_得分', '配套_得分', '学区_得分', '税费_得分', '描述丰富度', '核心卖点_特征', '房屋优势_长度', '核心卖点_长度', '周边配套_长度', '交通出行_长度', '综合文本得分', '房屋优势_长度_log', '核心卖点_长度_log', '周边配套_长度_log', '交通出行_长度_log', '综合文本得分_scaled', '描述丰富度_分档', '核心卖点_特征_encoded', '学区交通_交互', '配套描述_交互', '交通_关键词', '配套_关键词', '学区_关键词', '税费_关键词', '户型介绍_是否填写', '关键词匹配数量', '当前楼层', '总楼层', '多层住宅', '黄金楼层', '楼层占比', '楼层占比_平方', '楼层_电梯交互', '楼层_高层交互', '房龄', '面积_房龄', '房龄_平方', '单位租金_平方', '卧室_租金', '面积_租金']\n"}],"execution_count":47},{"cell_type":"code","metadata":{"id":"3A02CD0DDAE947658380F6CEB49FDBE2","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef find_optimal_k_clusters(data_scaled, k_range=None, max_k=15, min_samples_per_cluster=10,\n                           early_stop=True, patience=3, min_improvement=0.02):\n    \"\"\"\n    使用WSS（肘部法则）和轮廓系数来找到最优的聚类数K，支持早停机制\n    \n    参数:\n    ◦ data_scaled: 标准化后的数据\n    ◦ k_range: K值范围，如果为None则自动确定\n    ◦ max_k: 最大K值\n    ◦ min_samples_per_cluster: 每个簇的最少样本数\n    ◦ early_stop: 是否启用早停机制\n    ◦ patience: 早停耐心值（连续多少次没有改善就停止）\n    ◦ min_improvement: 最小改善阈值\n    \n    返回:\n    ◦ optimal_k: 最优K值\n    ◦ wss_scores: WSS得分列表\n    ◦ silhouette_scores: 轮廓系数列表\n    ◦ k_values: 尝试的K值列表\n    \"\"\"\n    \n    n_samples = len(data_scaled)\n    \n    # 自动确定K值范围\n    if k_range is None:\n        min_k = 2\n        # 确保每个簇至少有min_samples_per_cluster个样本\n        max_k_by_samples = max(2, n_samples // min_samples_per_cluster)\n        max_k = min(max_k, max_k_by_samples, 20)  # 限制最大值避免过度计算\n        k_range = range(min_k, max_k + 1)\n    \n    print(f\"寻找最优K值，范围: {list(k_range)}\")\n    if early_stop:\n        print(f\"早停设置: patience={patience}, min_improvement={min_improvement}\")\n    \n    wss_scores = []\n    silhouette_scores = []\n    k_values = []\n    \n    # 早停相关变量\n    best_silhouette = -1\n    no_improvement_count = 0\n    best_k_so_far = 2\n    \n    for k in k_range:\n        if k >= n_samples:\n            print(f\"K={k} 超过样本数，停止搜索\")\n            break\n            \n        try:\n            # 训练KMeans\n            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n            cluster_labels = kmeans.fit_predict(data_scaled)\n            \n            # 计算WSS（簇内平方和）\n            wss = kmeans.inertia_\n            \n            # 计算轮廓系数\n            if k > 1:\n                silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n            else:\n                silhouette_avg = 0\n            \n            wss_scores.append(wss)\n            silhouette_scores.append(silhouette_avg)\n            k_values.append(k)\n            \n            print(f\"K={k}: WSS={wss:.2f}, Silhouette={silhouette_avg:.3f}\")\n            \n            # 早停逻辑\n            if early_stop and k > 2:  # 至少尝试3个K值\n                improvement = silhouette_avg - best_silhouette\n                \n                if improvement > min_improvement:\n                    best_silhouette = silhouette_avg\n                    best_k_so_far = k\n                    no_improvement_count = 0\n                    print(f\"  ✓ 发现改善: +{improvement:.4f}\")\n                else:\n                    no_improvement_count += 1\n                    print(f\"  → 无显著改善 ({no_improvement_count}/{patience})\")\n                    \n                    # 检查是否需要早停\n                    if no_improvement_count >= patience:\n                        print(f\"🛑 早停触发: 连续{patience}次无改善，当前最佳K={best_k_so_far}\")\n                        # 如果WSS还在快速下降，给一次机会\n                        if len(wss_scores) >= 2:\n                            wss_decline_rate = (wss_scores[-2] - wss_scores[-1]) / wss_scores[-2]\n                            if wss_decline_rate > 0.1:  # WSS下降超过10%\n                                print(f\"  但WSS仍在快速下降({wss_decline_rate:.3f}), 继续搜索...\")\n                                no_improvement_count = patience - 1  # 重置但保持警戒\n                            else:\n                                break\n            \n        except Exception as e:\n            print(f\"K={k} 训练失败: {e}\")\n            continue\n    \n    if len(wss_scores) == 0:\n        print(\"警告: 无法计算任何K值的聚类效果\")\n        return 2, [], [], []\n    \n    # 使用肘部法则找到最优K\n    optimal_k = find_elbow_point(k_values, wss_scores)\n    \n    # 也可以考虑轮廓系数最高的K值作为参考\n    if len(silhouette_scores) > 0:\n        best_silhouette_k = k_values[np.argmax(silhouette_scores)]\n        print(f\"\\n📊 结果分析:\")\n        print(f\"肘部法则推荐K: {optimal_k}\")\n        print(f\"轮廓系数最佳K: {best_silhouette_k}\")\n        if early_stop:\n            print(f\"早停建议K: {best_k_so_far}\")\n        \n        # 综合决策逻辑\n        candidates = [optimal_k, best_silhouette_k]\n        if early_stop:\n            candidates.append(best_k_so_far)\n        \n        # 选择出现频率最高的K，或者轮廓系数最高的\n        from collections import Counter\n        k_counter = Counter(candidates)\n        most_common_k = k_counter.most_common(1)[0][0]\n        \n        if k_counter[most_common_k] > 1:\n            optimal_k = most_common_k\n            print(f\"最终选择K: {optimal_k} (多方法一致)\")\n        else:\n            optimal_k = best_silhouette_k\n            print(f\"最终选择K: {optimal_k} (优先轮廓系数)\")\n    \n    return optimal_k, wss_scores, silhouette_scores, k_values\n\ndef find_elbow_point(k_values, wss_scores):\n    \"\"\"\n    使用肘部法则找到最优K值\n    计算每个点到连接首末两点直线的距离，距离最大的点为肘部\n    \"\"\"\n    if len(wss_scores) <= 2:\n        return k_values[0] if k_values else 2\n    \n    # 将数据转换为numpy数组\n    k_array = np.array(k_values)\n    wss_array = np.array(wss_scores)\n    \n    # 标准化数据到[0,1]范围\n    k_norm = (k_array - k_array.min()) / (k_array.max() - k_array.min())\n    wss_norm = (wss_array - wss_array.min()) / (wss_array.max() - wss_array.min())\n    \n    # 计算每个点到首末连线的距离\n    first_point = np.array([k_norm[0], wss_norm[0]])\n    last_point = np.array([k_norm[-1], wss_norm[-1]])\n    \n    distances = []\n    for i in range(len(k_norm)):\n        point = np.array([k_norm[i], wss_norm[i]])\n        distance = point_to_line_distance(point, first_point, last_point)\n        distances.append(distance)\n    \n    # 找到距离最大的点\n    elbow_index = np.argmax(distances)\n    optimal_k = k_values[elbow_index]\n    \n    return optimal_k\n\ndef point_to_line_distance(point, line_start, line_end):\n    \"\"\"计算点到直线的距离\"\"\"\n    line_vec = line_end - line_start\n    point_vec = point - line_start\n    line_len = np.linalg.norm(line_vec)\n    \n    if line_len == 0:\n        return np.linalg.norm(point_vec)\n    \n    line_unitvec = line_vec / line_len\n    proj_length = np.dot(point_vec, line_unitvec)\n    proj = proj_length * line_unitvec\n    \n    return np.linalg.norm(point_vec - proj)\n\ndef plot_clustering_metrics(k_values, wss_scores, silhouette_scores, optimal_k):\n    \"\"\"绘制聚类评估指标图\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # WSS肘部图\n    ax1.plot(k_values, wss_scores, 'bo-')\n    ax1.axvline(x=optimal_k, color='red', linestyle='--', label=f'Optimal K={optimal_k}')\n    ax1.set_xlabel('Number of Clusters (K)')\n    ax1.set_ylabel('Within-cluster Sum of Squares (WSS)')\n    ax1.set_title('Elbow Method for Optimal K')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # 轮廓系数图\n    if len(silhouette_scores) > 0:\n        ax2.plot(k_values, silhouette_scores, 'go-')\n        ax2.axvline(x=optimal_k, color='red', linestyle='--', label=f'Selected K={optimal_k}')\n        ax2.set_xlabel('Number of Clusters (K)')\n        ax2.set_ylabel('Silhouette Score')\n        ax2.set_title('Silhouette Score for Different K')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef compute_enhanced_location_features_improved(train_df, test_df=None, predict_data=None, \n                                              min_k=5, max_k=15, lightweight_mode=True,\n                                              auto_select_clusters=True, plot_metrics=False,\n                                              early_stop=True, patience=3):\n    \"\"\"\n    改进版地理位置特征工程，使用WSS自动选择最优聚类数，支持早停\n    \n    新增参数:\n    ◦ auto_select_clusters: 是否自动选择聚类数\n    ◦ plot_metrics: 是否绘制聚类评估图\n    ◦ early_stop: 是否启用早停机制\n    ◦ patience: 早停耐心值\n    \"\"\"\n    \n    print(\"开始改进版地理位置特征工程...\")\n    \n    # 检查必要列\n    required_columns = ['价格', '建筑面积', 'lon', 'lat']\n    for col in required_columns:\n        if col not in train_df.columns:\n            raise ValueError(f\"训练集中未找到 '{col}' 列\")\n    \n    # 处理训练集\n    train_df = train_df.copy()\n    train_df[\"单价\"] = train_df[\"价格\"] / train_df[\"建筑面积\"]\n    train_locations = train_df[['lon', 'lat']].values\n    \n    print(f\"训练集样本数: {len(train_df)}\")\n    \n    # 标准化地理位置坐标\n    print(\"标准化地理位置坐标...\")\n    scaler = StandardScaler()\n    train_locations_scaled = scaler.fit_transform(train_locations)\n    \n    # 计算数据中心\n    data_center = np.mean(train_locations, axis=0)\n    print(f\"数据中心坐标: {data_center}\")\n    \n    # 构建KNN模型\n    print(\"构建KNN模型...\")\n    k_values = [5, 10, 15, 20] if not lightweight_mode else [5, 10]\n    knn_models = {}\n    \n    for k in k_values:\n        if k < len(train_df):\n            knn_models[k] = NearestNeighbors(n_neighbors=k, metric='euclidean')\n            knn_models[k].fit(train_locations_scaled)\n            print(f\"✓ KNN-{k} 模型训练完成\")\n    \n    # 聚类分析\n    cluster_model = None\n    optimal_k = None\n    \n    if auto_select_clusters:\n        print(\"自动选择最优聚类数...\")\n        optimal_k, wss_scores, silhouette_scores, k_values_tried = find_optimal_k_clusters(\n            train_locations_scaled, max_k=max_k, early_stop=early_stop, patience=patience\n        )\n        \n        if plot_metrics and len(wss_scores) > 0:\n            plot_clustering_metrics(k_values_tried, wss_scores, silhouette_scores, optimal_k)\n        \n        # 使用最优K训练聚类模型\n        if optimal_k and optimal_k < len(train_df):\n            cluster_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n            cluster_model.fit(train_locations_scaled)\n            print(f\"✓ 聚类模型训练完成 (K={optimal_k})\")\n    \n    # 为训练集添加特征\n    print(\"计算训练集特征...\")\n    train_df = _add_features_batch(\n        train_df, train_locations, train_locations_scaled, \n        knn_models, data_center, cluster_model, is_train=True\n    )\n    \n    result = {\n        'train_df': train_df,\n        'scaler': scaler,\n        'knn_models': knn_models,\n        'data_center': data_center,\n        'cluster_model': cluster_model,\n        'train_locations_scaled': train_locations_scaled,\n        'optimal_k': optimal_k\n    }\n    \n    # 处理验证集\n    if test_df is not None:\n        print(\"处理验证集...\")\n        test_df = test_df.copy()\n        if '价格' in test_df.columns and '建筑面积' in test_df.columns:\n            test_df[\"单价\"] = test_df[\"价格\"] / test_df[\"建筑面积\"]\n        \n        test_locations = test_df[['lon', 'lat']].values\n        test_locations_scaled = scaler.transform(test_locations)\n        \n        test_df = _add_features_batch(\n            test_df, test_locations, test_locations_scaled,\n            knn_models, data_center, cluster_model, \n            is_train=False, train_df=train_df\n        )\n        result['test_df'] = test_df\n    \n    # 处理预测集\n    if predict_data is not None:\n        print(\"处理预测集...\")\n        predict_data = predict_data.copy()\n        if '价格' in predict_data.columns and '建筑面积' in predict_data.columns:\n            predict_data[\"单价\"] = predict_data[\"价格\"] / predict_data[\"建筑面积\"]\n        \n        predict_locations = predict_data[['lon', 'lat']].values\n        predict_locations_scaled = scaler.transform(predict_locations)\n        \n        predict_data = _add_features_batch(\n            predict_data, predict_locations, predict_locations_scaled,\n            knn_models, data_center, cluster_model,\n            is_train=False, train_df=train_df\n        )\n        result['predict_data'] = predict_data\n    \n    print(\"特征工程完成！\")\n    \n    # 检查新特征是否正确添加\n    check_new_features(result['train_df'], \"训练集\")\n    if 'test_df' in result:\n        check_new_features(result['test_df'], \"验证集\")\n    if 'predict_data' in result:\n        check_new_features(result['predict_data'], \"预测集\")\n    \n    return result\n\ndef _add_features_batch(df, locations, locations_scaled, knn_models, \n                       data_center, cluster_model, is_train=True, train_df=None):\n    \"\"\"批量添加特征，优化内存使用\"\"\"\n    \n    reference_df = df if is_train else train_df\n    batch_size = 3000  # 分批处理，避免内存溢出\n    total_samples = len(df)\n    \n    # 初始化特征列表\n    features = {\n        '地理位置评分': [],\n        '到数据中心距离': [],\n        '密度_1km': []\n    }\n    \n    # 为每个K值初始化KNN特征\n    for k in knn_models.keys():\n        features[f'knn_{k}_mean'] = []\n        features[f'knn_{k}_std'] = []\n    \n    # 如果有单价信息，添加排名特征\n    if '单价' in df.columns:\n        features['邻域价格排名'] = []\n    \n    # 聚类特征\n    if cluster_model:\n        features['cluster_id'] = []\n        features['cluster_avg_price'] = []\n    \n    print(f\"分批处理 {total_samples} 个样本...\")\n    \n    # 分批处理\n    for start_idx in range(0, total_samples, batch_size):\n        end_idx = min(start_idx + batch_size, total_samples)\n        batch_locations = locations[start_idx:end_idx]\n        batch_locations_scaled = locations_scaled[start_idx:end_idx]\n        \n        print(f\"处理批次 {start_idx}-{end_idx}...\")\n        \n        # 1. 基础地理位置评分\n        if 5 in knn_models:\n            distances, indices = knn_models[5].kneighbors(batch_locations_scaled)\n            batch_scores = []\n            for i, idx_list in enumerate(indices):\n                neighbor_prices = reference_df.iloc[idx_list][\"单价\"].values\n                batch_scores.append(np.mean(neighbor_prices))\n            features['地理位置评分'].extend(batch_scores)\n        \n        # 2. 到数据中心距离\n        center_distances = np.sqrt(np.sum((batch_locations - data_center) ** 2, axis=1))\n        features['到数据中心距离'].extend(center_distances.tolist())\n        \n        # 3. 简化的密度特征（只计算1km）\n        reference_locations = reference_df[['lon', 'lat']].values\n        radius_deg = 1.0 / 111.0  # 1km转换为度\n        \n        batch_densities = []\n        for loc in batch_locations:\n            distances_to_ref = np.sqrt(np.sum((reference_locations - loc) ** 2, axis=1))\n            density = np.sum(distances_to_ref <= radius_deg)\n            batch_densities.append(density)\n        features['密度_1km'].extend(batch_densities)\n        \n        # 4. KNN统计特征\n        for k in knn_models.keys():\n            distances, indices = knn_models[k].kneighbors(batch_locations_scaled)\n            \n            batch_means = []\n            batch_stds = []\n            \n            for idx_list in indices:\n                neighbor_prices = reference_df.iloc[idx_list][\"单价\"].values\n                batch_means.append(np.mean(neighbor_prices))\n                batch_stds.append(np.std(neighbor_prices))\n            \n            features[f'knn_{k}_mean'].extend(batch_means)\n            features[f'knn_{k}_std'].extend(batch_stds)\n        \n        # 5. 邻域价格排名（如果有单价）\n        if '单价' in df.columns and 10 in knn_models:\n            distances, indices = knn_models[10].kneighbors(batch_locations_scaled)\n            batch_rankings = []\n            \n            for i, idx_list in enumerate(indices):\n                current_price = df.iloc[start_idx + i][\"单价\"]\n                neighbor_prices = reference_df.iloc[idx_list][\"单价\"].values\n                rank = np.sum(neighbor_prices < current_price) / len(neighbor_prices)\n                batch_rankings.append(rank)\n            \n            features['邻域价格排名'].extend(batch_rankings)\n        \n        # 6. 聚类特征\n        if cluster_model:\n            batch_clusters = cluster_model.predict(batch_locations_scaled)\n            features['cluster_id'].extend(batch_clusters.tolist())\n            \n            # 计算簇平均价格\n            batch_cluster_prices = []\n            train_clusters = cluster_model.predict(reference_df[['lon', 'lat']].values \n                                                if hasattr(reference_df, 'values') \n                                                else locations_scaled if is_train \n                                                else train_df[['lon', 'lat']].values)\n            \n            for cluster_id in batch_clusters:\n                same_cluster_mask = train_clusters == cluster_id\n                if same_cluster_mask.sum() > 0:\n                    cluster_price = reference_df[same_cluster_mask][\"单价\"].mean()\n                else:\n                    cluster_price = reference_df[\"单价\"].mean()\n                batch_cluster_prices.append(cluster_price)\n            \n            features['cluster_avg_price'].extend(batch_cluster_prices)\n    \n    # 将特征添加到DataFrame\n    print(f\"添加 {len(features)} 个新特征到DataFrame...\")\n    for feature_name, feature_values in features.items():\n        if len(feature_values) == len(df):\n            df[feature_name] = feature_values\n            print(f\"✓ 已添加特征: {feature_name}\")\n        else:\n            print(f\"✗ 特征长度不匹配: {feature_name} (期望{len(df)}, 实际{len(feature_values)})\")\n    \n    print(f\"DataFrame现在有 {len(df.columns)} 列\")\n    return df\n\ndef check_new_features(df, dataset_name=\"数据集\"):\n    \"\"\"检查新增的地理位置特征\"\"\"\n    location_features = [col for col in df.columns if any(keyword in col for keyword in \n                        ['地理位置', 'knn_', '密度', '距离', 'cluster'])]\n    \n    print(f\"\\n{dataset_name}新增特征检查:\")\n    print(f\"总列数: {len(df.columns)}\")\n    print(f\"地理特征数: {len(location_features)}\")\n    print(\"地理特征列表:\")\n    for i, feature in enumerate(location_features, 1):\n        sample_values = df[feature].head(3).values\n        print(f\"  {i}. {feature}: {sample_values}\")\n    \n    return location_features\n\ndef print_feature_summary(train_df, test_df=None, predict_df=None):\n    \"\"\"打印特征摘要\"\"\"\n    \n    location_features = [col for col in train_df.columns if any(keyword in col for keyword in \n                        ['地理位置', 'knn_', '密度', '距离', 'cluster'])]\n    \n    print(\"=\" * 50)\n    print(\"地理位置特征工程摘要\")\n    print(\"=\" * 50)\n    print(f\"训练集: {len(train_df)} 样本\")\n    print(f\"新增地理特征: {len(location_features)} 个\")\n    print(\"特征列表:\", location_features)\n    \n    if test_df is not None:\n        print(f\"验证集: {len(test_df)} 样本\")\n    if predict_df is not None:\n        print(f\"预测集: {len(predict_df)} 样本\")\n    \n    # 关键特征统计\n    key_features = ['地理位置评分', 'knn_10_mean', '到数据中心距离']\n    print(\"\\n关键特征统计:\")\n    for feature in key_features:\n        if feature in train_df.columns:\n            mean_val = train_df[feature].mean()\n            std_val = train_df[feature].std()\n            print(f\"{feature}: 均值={mean_val:.3f}, 标准差={std_val:.3f}\")\n\n# 使用示例\nif __name__ == \"__main__\":\n    result = compute_enhanced_location_features_improved(\n        train_df, test_df, predict_data,\n        auto_select_clusters=True,\n        early_stop=True,      # 启用早停\n        patience=3,           # 耐心值\n        max_k=20,            # 仍然设置最大K值作为硬限制\n        plot_metrics=True)","outputs":[{"output_type":"stream","name":"stdout","text":"开始改进版地理位置特征工程...\n训练集样本数: 71458\n标准化地理位置坐标...\n数据中心坐标: [113.36843402  35.2449446 ]\n构建KNN模型...\n✓ KNN-5 模型训练完成\n✓ KNN-10 模型训练完成\n自动选择最优聚类数...\n寻找最优K值，范围: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n早停设置: patience=3, min_improvement=0.02\nK=2: WSS=44281.32, Silhouette=0.661\nK=3: WSS=21685.15, Silhouette=0.726\n  ✓ 发现改善: +1.7262\nK=4: WSS=9711.92, Silhouette=0.792\n  ✓ 发现改善: +0.0655\nK=5: WSS=2680.00, Silhouette=0.921\n  ✓ 发现改善: +0.1295\nK=6: WSS=63.74, Silhouette=0.972\n  ✓ 发现改善: +0.0505\nK=7: WSS=49.14, Silhouette=0.839\n  → 无显著改善 (1/3)\nK=8: WSS=42.70, Silhouette=0.848\n  → 无显著改善 (2/3)\nK=9: WSS=38.17, Silhouette=0.763\n  → 无显著改善 (3/3)\n🛑 早停触发: 连续3次无改善，当前最佳K=6\n  但WSS仍在快速下降(0.106), 继续搜索...\nK=10: WSS=34.66, Silhouette=0.647\n  → 无显著改善 (3/3)\n🛑 早停触发: 连续3次无改善，当前最佳K=6\n\n📊 结果分析:\n肘部法则推荐K: 5\n轮廓系数最佳K: 6\n早停建议K: 6\n最终选择K: 6 (多方法一致)\n"},{"output_type":"display_data","data":{"text/plain":"<Figure size 864x288 with 2 Axes>","text/html":"<img src=\"/s3/static-files/upload/rt/A04FEF6199B44F70991E58E33A31E8BF/sx6khx7hgf.png\">"},"metadata":{}},{"output_type":"stream","name":"stdout","text":"✓ 聚类模型训练完成 (K=6)\n计算训练集特征...\n分批处理 71458 个样本...\n处理批次 0-3000...\n处理批次 3000-6000...\n处理批次 6000-9000...\n处理批次 9000-12000...\n处理批次 12000-15000...\n处理批次 15000-18000...\n处理批次 18000-21000...\n处理批次 21000-24000...\n处理批次 24000-27000...\n处理批次 27000-30000...\n处理批次 30000-33000...\n处理批次 33000-36000...\n处理批次 36000-39000...\n处理批次 39000-42000...\n处理批次 42000-45000...\n处理批次 45000-48000...\n处理批次 48000-51000...\n处理批次 51000-54000...\n处理批次 54000-57000...\n处理批次 57000-60000...\n处理批次 60000-63000...\n处理批次 63000-66000...\n处理批次 66000-69000...\n处理批次 69000-71458...\n添加 10 个新特征到DataFrame...\n✓ 已添加特征: 地理位置评分\n✓ 已添加特征: 到数据中心距离\n✓ 已添加特征: 密度_1km\n✓ 已添加特征: knn_5_mean\n✓ 已添加特征: knn_5_std\n✓ 已添加特征: knn_10_mean\n✓ 已添加特征: knn_10_std\n✓ 已添加特征: 邻域价格排名\n✓ 已添加特征: cluster_id\n✓ 已添加特征: cluster_avg_price\nDataFrame现在有 195 列\n处理验证集...\n分批处理 12619 个样本...\n处理批次 0-3000...\n处理批次 3000-6000...\n处理批次 6000-9000...\n处理批次 9000-12000...\n处理批次 12000-12619...\n添加 10 个新特征到DataFrame...\n✓ 已添加特征: 地理位置评分\n✓ 已添加特征: 到数据中心距离\n✓ 已添加特征: 密度_1km\n✓ 已添加特征: knn_5_mean\n✓ 已添加特征: knn_5_std\n✓ 已添加特征: knn_10_mean\n✓ 已添加特征: knn_10_std\n✓ 已添加特征: 邻域价格排名\n✓ 已添加特征: cluster_id\n✓ 已添加特征: cluster_avg_price\nDataFrame现在有 195 列\n处理预测集...\n分批处理 14786 个样本...\n处理批次 0-3000...\n处理批次 3000-6000...\n处理批次 6000-9000...\n处理批次 9000-12000...\n处理批次 12000-14786...\n添加 9 个新特征到DataFrame...\n✓ 已添加特征: 地理位置评分\n✓ 已添加特征: 到数据中心距离\n✓ 已添加特征: 密度_1km\n✓ 已添加特征: knn_5_mean\n✓ 已添加特征: knn_5_std\n✓ 已添加特征: knn_10_mean\n✓ 已添加特征: knn_10_std\n✓ 已添加特征: cluster_id\n✓ 已添加特征: cluster_avg_price\nDataFrame现在有 192 列\n特征工程完成！\n\n训练集新增特征检查:\n总列数: 195\n地理特征数: 9\n地理特征列表:\n  1. 地理位置评分: [13361.79451279 49370.00778244 12966.46395218]\n  2. 到数据中心距离: [4.78593697 5.58215745 8.84884019]\n  3. 密度_1km: [ 70 147 404]\n  4. knn_5_mean: [13361.79451279 49370.00778244 12966.46395218]\n  5. knn_5_std: [ 1558.7278863  21437.29513417  1171.04161518]\n  6. knn_10_mean: [13546.84160322 51734.59352917 12854.46340351]\n  7. knn_10_std: [ 1435.59428707 15511.25917674  1046.03277977]\n  8. cluster_id: [5 2 0]\n  9. cluster_avg_price: [20463.89297865 20463.89297865 20463.89297865]\n\n验证集新增特征检查:\n总列数: 195\n地理特征数: 9\n地理特征列表:\n  1. 地理位置评分: [14864.69252297 10611.01003626 70734.62577404]\n  2. 到数据中心距离: [5.84748873 8.84327535 5.64852735]\n  3. 密度_1km: [346 149 200]\n  4. knn_5_mean: [14864.69252297 10611.01003626 70734.62577404]\n  5. knn_5_std: [ 3033.85832874  1139.15620027 18985.35639738]\n  6. knn_10_mean: [16541.32890411 10750.27528013 73873.20408864]\n  7. knn_10_std: [ 3095.98478567   852.11501935 16712.5924516 ]\n  8. cluster_id: [2 0 2]\n  9. cluster_avg_price: [20463.89297865 20463.89297865 20463.89297865]\n\n预测集新增特征检查:\n总列数: 192\n地理特征数: 9\n地理特征列表:\n  1. 地理位置评分: [111778.7379233   65583.90573657  32441.74400697]\n  2. 到数据中心距离: [5.65613643 5.68246739 5.7222215 ]\n  3. 密度_1km: [ 64 219 392]\n  4. knn_5_mean: [111778.7379233   65583.90573657  32441.74400697]\n  5. knn_5_std: [6273.37487339 3026.84370372 3338.95037645]\n  6. knn_10_mean: [100391.10569209  67351.49941808  30611.74132012]\n  7. knn_10_std: [12400.17975905  3623.08786158  4900.21817611]\n  8. cluster_id: [2 2 2]\n  9. cluster_avg_price: [20463.89297865 20463.89297865 20463.89297865]\n"}],"execution_count":48},{"cell_type":"code","metadata":{"id":"B17E3422F4F444D4A31E06DE92188133","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef compute_enhanced_location_features(train_df, test_df=None, predict_data=None, \n                                     min_k=5, max_k=15, lightweight_mode=True):\n    \"\"\"\n    轻量级增强版地理位置特征工程\n    \n    参数:\n    - train_df: 训练集DataFrame\n    - test_df: 验证集DataFrame（可选）\n    - predict_data: 预测集DataFrame（可选）\n    - min_k, max_k: KNN参数\n    - lightweight_mode: 轻量级模式，减少内存使用\n    \"\"\"\n    \n    print(\"开始地理位置特征工程...\")\n    \n    # 检查必要列\n    required_columns = ['价格', '建筑面积', 'lon', 'lat']\n    for col in required_columns:\n        if col not in train_df.columns:\n            raise ValueError(f\"训练集中未找到 '{col}' 列\")\n    \n    # 处理训练集\n    train_df = train_df.copy()\n    train_df[\"单价\"] = train_df[\"价格\"] / train_df[\"建筑面积\"]\n    train_locations = train_df[['lon', 'lat']].values\n    \n    print(f\"训练集样本数: {len(train_df)}\")\n    \n    # 标准化和KNN\n    scaler = StandardScaler()\n    train_locations_scaled = scaler.fit_transform(train_locations)\n    \n    # 只使用必要的K值，减少内存占用\n    k_values = [5, 10] if lightweight_mode else [5, 10, 20]\n    knn_models = {}\n    \n    for k in k_values:\n        actual_k = min(k, len(train_df) - 1)\n        if actual_k > 0:\n            print(f\"训练KNN模型 K={actual_k}...\")\n            knn_models[k] = NearestNeighbors(n_neighbors=actual_k, algorithm='ball_tree')\n            knn_models[k].fit(train_locations_scaled)\n    \n    # 计算数据中心\n    data_center = np.mean(train_locations, axis=0)\n    \n    # 简化聚类（只用一个小的K-means）\n    cluster_model = None\n    if len(train_df) > 20 and not lightweight_mode:\n        try:\n            print(\"训练聚类模型...\")\n            n_clusters = min(10, len(train_df) // 5)  # 动态调整簇数\n            cluster_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=5)\n            cluster_model.fit(train_locations_scaled)\n        except Exception as e:\n            print(f\"聚类训练失败，跳过: {e}\")\n    \n    # 为训练集添加特征\n    print(\"计算训练集特征...\")\n    train_df = _add_features_batch(\n        train_df, train_locations, train_locations_scaled, \n        knn_models, data_center, cluster_model, is_train=True\n    )\n    \n    result = {\n        'train_df': train_df,\n        'scaler': scaler,\n        'knn_models': knn_models,\n        'data_center': data_center,\n        'cluster_model': cluster_model,\n        'train_locations_scaled': train_locations_scaled\n    }\n    \n    # 处理验证集\n    if test_df is not None:\n        print(\"处理验证集...\")\n        test_df = test_df.copy()\n        if '价格' in test_df.columns and '建筑面积' in test_df.columns:\n            test_df[\"单价\"] = test_df[\"价格\"] / test_df[\"建筑面积\"]\n        \n        test_locations = test_df[['lon', 'lat']].values\n        test_locations_scaled = scaler.transform(test_locations)\n        \n        test_df = _add_features_batch(\n            test_df, test_locations, test_locations_scaled,\n            knn_models, data_center, cluster_model, \n            is_train=False, train_df=train_df\n        )\n        result['test_df'] = test_df\n    \n    # 处理预测集\n    if predict_data is not None:\n        print(\"处理预测集...\")\n        predict_data = predict_data.copy()\n        if '价格' in predict_data.columns and '建筑面积' in predict_data.columns:\n            predict_data[\"单价\"] = predict_data[\"价格\"] / predict_data[\"建筑面积\"]\n        \n        predict_locations = predict_data[['lon', 'lat']].values\n        predict_locations_scaled = scaler.transform(predict_locations)\n        \n        predict_data = _add_features_batch(\n            predict_data, predict_locations, predict_locations_scaled,\n            knn_models, data_center, cluster_model,\n            is_train=False, train_df=train_df\n        )\n        result['predict_data'] = predict_data\n    \n    print(\"特征工程完成！\")\n    \n    # 检查新特征是否正确添加\n    check_new_features(result['train_df'], \"训练集\")\n    if 'test_df' in result:\n        check_new_features(result['test_df'], \"验证集\")\n    if 'predict_data' in result:\n        check_new_features(result['predict_data'], \"预测集\")\n    \n    return result\n\ndef _add_features_batch(df, locations, locations_scaled, knn_models, \n                       data_center, cluster_model, is_train=True, train_df=None):\n    \"\"\"批量添加特征，优化内存使用\"\"\"\n    \n    reference_df = df if is_train else train_df\n    batch_size = 3000  # 分批处理，避免内存溢出\n    total_samples = len(df)\n    \n    # 初始化特征列表\n    features = {\n        '地理位置评分': [],\n        '到数据中心距离': [],\n        '密度_1km': []\n    }\n    \n    # 为每个K值初始化KNN特征\n    for k in knn_models.keys():\n        features[f'knn_{k}_mean'] = []\n        features[f'knn_{k}_std'] = []\n    \n    # 如果有单价信息，添加排名特征\n    if '单价' in df.columns:\n        features['邻域价格排名'] = []\n    \n    # 聚类特征\n    if cluster_model:\n        features['cluster_id'] = []\n        features['cluster_avg_price'] = []\n    \n    print(f\"分批处理 {total_samples} 个样本...\")\n    \n    # 分批处理\n    for start_idx in range(0, total_samples, batch_size):\n        end_idx = min(start_idx + batch_size, total_samples)\n        batch_locations = locations[start_idx:end_idx]\n        batch_locations_scaled = locations_scaled[start_idx:end_idx]\n        \n        print(f\"处理批次 {start_idx}-{end_idx}...\")\n        \n        # 1. 基础地理位置评分\n        if 5 in knn_models:\n            distances, indices = knn_models[5].kneighbors(batch_locations_scaled)\n            batch_scores = []\n            for i, idx_list in enumerate(indices):\n                neighbor_prices = reference_df.iloc[idx_list][\"单价\"].values\n                batch_scores.append(np.mean(neighbor_prices))\n            features['地理位置评分'].extend(batch_scores)\n        \n        # 2. 到数据中心距离\n        center_distances = np.sqrt(np.sum((batch_locations - data_center) ** 2, axis=1))\n        features['到数据中心距离'].extend(center_distances.tolist())\n        \n        # 3. 简化的密度特征（只计算1km）\n        reference_locations = reference_df[['lon', 'lat']].values\n        radius_deg = 1.0 / 111.0  # 1km转换为度\n        \n        batch_densities = []\n        for loc in batch_locations:\n            distances_to_ref = np.sqrt(np.sum((reference_locations - loc) ** 2, axis=1))\n            density = np.sum(distances_to_ref <= radius_deg)\n            batch_densities.append(density)\n        features['密度_1km'].extend(batch_densities)\n        \n        # 4. KNN统计特征\n        for k in knn_models.keys():\n            distances, indices = knn_models[k].kneighbors(batch_locations_scaled)\n            \n            batch_means = []\n            batch_stds = []\n            \n            for idx_list in indices:\n                neighbor_prices = reference_df.iloc[idx_list][\"单价\"].values\n                batch_means.append(np.mean(neighbor_prices))\n                batch_stds.append(np.std(neighbor_prices))\n            \n            features[f'knn_{k}_mean'].extend(batch_means)\n            features[f'knn_{k}_std'].extend(batch_stds)\n        \n        # 5. 价格排名特征\n        if '单价' in df.columns and 10 in knn_models:\n            distances, indices = knn_models[10].kneighbors(batch_locations_scaled)\n            batch_rankings = []\n            \n            for i, idx_list in enumerate(indices):\n                neighbor_prices = reference_df.iloc[idx_list][\"单价\"].values\n                current_price = df.iloc[start_idx + i][\"单价\"]\n                rank = (neighbor_prices < current_price).sum() / len(neighbor_prices)\n                batch_rankings.append(rank)\n            \n            features['邻域价格排名'].extend(batch_rankings)\n        \n        # 6. 聚类特征\n        if cluster_model:\n            batch_clusters = cluster_model.predict(batch_locations_scaled)\n            features['cluster_id'].extend(batch_clusters.tolist())\n            \n            # 计算簇平均价格\n            batch_cluster_prices = []\n            for cluster_id in batch_clusters:\n                train_clusters = cluster_model.predict(locations_scaled if is_train else \n                                                    cluster_model.cluster_centers_)\n                same_cluster_mask = train_clusters == cluster_id\n                if same_cluster_mask.sum() > 0:\n                    cluster_price = reference_df[same_cluster_mask][\"单价\"].mean()\n                else:\n                    cluster_price = reference_df[\"单价\"].mean()\n                batch_cluster_prices.append(cluster_price)\n            \n            features['cluster_avg_price'].extend(batch_cluster_prices)\n    \n    # 将特征添加到DataFrame\n    print(f\"添加 {len(features)} 个新特征到DataFrame...\")\n    for feature_name, feature_values in features.items():\n        if len(feature_values) == len(df):\n            df[feature_name] = feature_values\n            print(f\"✓ 已添加特征: {feature_name}\")\n        else:\n            print(f\"✗ 特征长度不匹配: {feature_name} (期望{len(df)}, 实际{len(feature_values)})\")\n    \n    print(f\"DataFrame现在有 {len(df.columns)} 列\")\n    return df\n\ndef check_new_features(df, dataset_name=\"数据集\"):\n    \"\"\"检查新增的地理位置特征\"\"\"\n    location_features = [col for col in df.columns if any(keyword in col for keyword in \n                        ['地理位置', 'knn_', '密度', '距离', '排名', 'cluster'])]\n    \n    print(f\"\\n{dataset_name}新增特征检查:\")\n    print(f\"总列数: {len(df.columns)}\")\n    print(f\"地理特征数: {len(location_features)}\")\n    print(\"地理特征列表:\")\n    for i, feature in enumerate(location_features, 1):\n        sample_values = df[feature].head(3).values\n        print(f\"  {i}. {feature}: {sample_values}\")\n    \n    return location_features\n\ndef print_feature_summary(train_df, test_df=None, predict_df=None):\n    \"\"\"打印特征摘要\"\"\"\n    \n    location_features = [col for col in train_df.columns if any(keyword in col for keyword in \n                        ['地理位置', 'knn_', '密度', '距离', '排名', 'cluster'])]\n    \n    print(\"=\" * 50)\n    print(\"地理位置特征工程摘要\")\n    print(\"=\" * 50)\n    print(f\"训练集: {len(train_df)} 样本\")\n    print(f\"新增地理特征: {len(location_features)} 个\")\n    print(\"特征列表:\", location_features)\n    \n    if test_df is not None:\n        print(f\"验证集: {len(test_df)} 样本\")\n    if predict_df is not None:\n        print(f\"预测集: {len(predict_df)} 样本\")\n    \n    # 关键特征统计\n    key_features = ['地理位置评分', 'knn_10_mean', '到数据中心距离']\n    print(\"\\n关键特征统计:\")\n    for feature in key_features:\n        if feature in train_df.columns:\n            mean_val = train_df[feature].mean()\n            std_val = train_df[feature].std()\n            print(f\"{feature}: 均值={mean_val:.3f}, 标准差={std_val:.3f}\")\n\n# 使用示例\nif __name__ == \"__main__\":\n    print(\"轻量级地理位置特征工程已准备就绪！\")\n    print(\"\\n使用方法:\")\n    print(\"# 轻量级模式（推荐）\")\n    result = compute_enhanced_location_features(train_df, test_df, predict_data, lightweight_mode=True)\n    print(\"\\n# 检查新特征\")\n    enhanced_train = result['train_df']\n    print('新增特征:', [col for col in enhanced_train.columns if 'knn_' in col or '地理位置' in col])\n    print('训练集形状:', enhanced_train.shape)\n    print(\"\\n# 查看结果摘要\")\n    print_feature_summary(result['train_df'], result.get('test_df'))\n    \n    print(\"\\n🔍 特征检查方法:\")\n    print(\"# 直接查看DataFrame的列\")\n    print(enhanced_train.columns.tolist())\n    print(\"# 查看特定特征的值\")\n    print(enhanced_train[['地理位置评分', 'knn_5_mean', '到数据中心距离']].head())","outputs":[{"output_type":"stream","name":"stdout","text":"轻量级地理位置特征工程已准备就绪！\n\n使用方法:\n# 轻量级模式（推荐）\n开始地理位置特征工程...\n训练集样本数: 71458\n训练KNN模型 K=5...\n训练KNN模型 K=10...\n计算训练集特征...\n分批处理 71458 个样本...\n处理批次 0-3000...\n处理批次 3000-6000...\n处理批次 6000-9000...\n处理批次 9000-12000...\n处理批次 12000-15000...\n处理批次 15000-18000...\n处理批次 18000-21000...\n处理批次 21000-24000...\n处理批次 24000-27000...\n处理批次 27000-30000...\n处理批次 30000-33000...\n处理批次 33000-36000...\n处理批次 36000-39000...\n处理批次 39000-42000...\n处理批次 42000-45000...\n处理批次 45000-48000...\n处理批次 48000-51000...\n处理批次 51000-54000...\n处理批次 54000-57000...\n处理批次 57000-60000...\n处理批次 60000-63000...\n处理批次 63000-66000...\n处理批次 66000-69000...\n处理批次 69000-71458...\n添加 8 个新特征到DataFrame...\n✓ 已添加特征: 地理位置评分\n✓ 已添加特征: 到数据中心距离\n✓ 已添加特征: 密度_1km\n✓ 已添加特征: knn_5_mean\n✓ 已添加特征: knn_5_std\n✓ 已添加特征: knn_10_mean\n✓ 已添加特征: knn_10_std\n✓ 已添加特征: 邻域价格排名\nDataFrame现在有 173 列\n处理验证集...\n分批处理 12619 个样本...\n处理批次 0-3000...\n处理批次 3000-6000...\n处理批次 6000-9000...\n处理批次 9000-12000...\n处理批次 12000-12619...\n添加 8 个新特征到DataFrame...\n✓ 已添加特征: 地理位置评分\n✓ 已添加特征: 到数据中心距离\n✓ 已添加特征: 密度_1km\n✓ 已添加特征: knn_5_mean\n✓ 已添加特征: knn_5_std\n✓ 已添加特征: knn_10_mean\n✓ 已添加特征: knn_10_std\n✓ 已添加特征: 邻域价格排名\nDataFrame现在有 173 列\n处理预测集...\n分批处理 14786 个样本...\n处理批次 0-3000...\n处理批次 3000-6000...\n处理批次 6000-9000...\n处理批次 9000-12000...\n处理批次 12000-14786...\n添加 7 个新特征到DataFrame...\n✓ 已添加特征: 地理位置评分\n✓ 已添加特征: 到数据中心距离\n✓ 已添加特征: 密度_1km\n✓ 已添加特征: knn_5_mean\n✓ 已添加特征: knn_5_std\n✓ 已添加特征: knn_10_mean\n✓ 已添加特征: knn_10_std\nDataFrame现在有 170 列\n特征工程完成！\n\n训练集新增特征检查:\n总列数: 173\n地理特征数: 8\n地理特征列表:\n  1. 地理位置评分: [13361.79451279 49370.00778244 12966.46395218]\n  2. 到数据中心距离: [4.78593697 5.58215745 8.84884019]\n  3. 密度_1km: [ 70 147 404]\n  4. knn_5_mean: [13361.79451279 49370.00778244 12966.46395218]\n  5. knn_5_std: [ 1558.7278863  21437.29513417  1171.04161518]\n  6. knn_10_mean: [13546.84160322 51734.59352917 12854.46340351]\n  7. knn_10_std: [ 1435.59428707 15511.25917674  1046.03277977]\n  8. 邻域价格排名: [0.9 0.  0.4]\n\n验证集新增特征检查:\n总列数: 173\n地理特征数: 8\n地理特征列表:\n  1. 地理位置评分: [14864.69252297 10611.01003626 70734.62577404]\n  2. 到数据中心距离: [5.84748873 8.84327535 5.64852735]\n  3. 密度_1km: [346 149 200]\n  4. knn_5_mean: [14864.69252297 10611.01003626 70734.62577404]\n  5. knn_5_std: [ 3033.85832874  1139.15620027 18985.35639738]\n  6. knn_10_mean: [16541.32890411 10750.27528013 73873.20408864]\n  7. knn_10_std: [ 3095.98478567   852.11501935 16712.5924516 ]\n  8. 邻域价格排名: [0.4 0.1 0.5]\n\n预测集新增特征检查:\n总列数: 170\n地理特征数: 7\n地理特征列表:\n  1. 地理位置评分: [111778.7379233   65583.90573657  32441.74400697]\n  2. 到数据中心距离: [5.65613643 5.68246739 5.7222215 ]\n  3. 密度_1km: [ 64 219 392]\n  4. knn_5_mean: [111778.7379233   65583.90573657  32441.74400697]\n  5. knn_5_std: [6273.37487339 3026.84370372 3338.95037645]\n  6. knn_10_mean: [100391.10569209  67351.49941808  30611.74132012]\n  7. knn_10_std: [12400.17975905  3623.08786158  4900.21817611]\n\n# 检查新特征\n新增特征: ['地理位置评分', 'knn_5_mean', 'knn_5_std', 'knn_10_mean', 'knn_10_std']\n训练集形状: (71458, 173)\n\n# 查看结果摘要\n==================================================\n地理位置特征工程摘要\n==================================================\n训练集: 71458 样本\n新增地理特征: 8 个\n特征列表: ['地理位置评分', '到数据中心距离', '密度_1km', 'knn_5_mean', 'knn_5_std', 'knn_10_mean', 'knn_10_std', '邻域价格排名']\n验证集: 12619 样本\n\n关键特征统计:\n地理位置评分: 均值=20463.652, 标准差=20312.296\nknn_10_mean: 均值=20436.324, 标准差=20172.316\n到数据中心距离: 均值=7.854, 标准差=3.797\n\n🔍 特征检查方法:\n# 直接查看DataFrame的列\n['城市', '区域', '板块', '环线', '小区名称', '价格', '房屋户型', '所在楼层', '建筑面积', '梯户比例', '配备电梯', '房屋优势', '核心卖点', '户型介绍', '周边配套', '交通出行', 'lon', 'lat', '匹配小区', '建筑年代', '房屋总数', '楼栋总数', '绿 化 率', '容 积 率', '物 业 费', '停车位', '停车费用', '平均每平米月租金', '交易年份', '建筑结构编码_混合结构', '建筑结构编码_钢混结构', '装修编码_毛坯', '装修编码_简装', '装修编码_精装', '房屋数量', '楼栋数量', '绿化率', '容积率', '物业费', '停车位数量', '停车费', 'poly_房屋数量', 'poly_楼栋数量', 'poly_绿化率', 'poly_容积率', 'poly_物业费', 'poly_停车位数量', 'poly_停车费', 'poly_房屋数量^2', 'poly_房屋数量 楼栋数量', 'poly_房屋数量 绿化率', 'poly_房屋数量 容积率', 'poly_房屋数量 物业费', 'poly_房屋数量 停车位数量', 'poly_房屋数量 停车费', 'poly_楼栋数量^2', 'poly_楼栋数量 绿化率', 'poly_楼栋数量 容积率', 'poly_楼栋数量 物业费', 'poly_楼栋数量 停车位数量', 'poly_楼栋数量 停车费', 'poly_绿化率^2', 'poly_绿化率 容积率', 'poly_绿化率 物业费', 'poly_绿化率 停车位数量', 'poly_绿化率 停车费', 'poly_容积率^2', 'poly_容积率 物业费', 'poly_容积率 停车位数量', 'poly_容积率 停车费', 'poly_物业费^2', 'poly_物业费 停车位数量', 'poly_物业费 停车费', 'poly_停车位数量^2', 'poly_停车位数量 停车费', 'poly_停车费^2', 'area_scaled', 'area_squared', '主要朝向_北', '主要朝向_南', '主要朝向_西', '环线_填补', '城市_环线_0_二环内', '城市_环线_0_二至三环', '城市_环线_0_五至六环', '城市_环线_0_六环外', '城市_环线_0_四至五环', '城市_环线_0_无', '城市_环线_1_无环线', '城市_环线_2_内环内', '城市_环线_2_内环至外环', '城市_环线_2_外环外', '城市_环线_2_无', '城市_环线_3_中环至外环', '城市_环线_3_内环内', '城市_环线_3_内环至中环', '城市_环线_3_外环外', '城市_环线_3_无', '城市_环线_4_一环内', '城市_环线_4_一至二环', '城市_环线_4_三至四环', '城市_环线_4_二至三环', '城市_环线_4_四环外', '城市_环线_4_无', '城市_环线_5_无环线', '城市_环线_6_一环内', '城市_环线_6_一至二环', '城市_环线_6_三环外', '城市_环线_6_二至三环', '城市_环线_6_无', '城市_环线_0_二环内_建筑面积', '城市_环线_0_二至三环_建筑面积', '城市_环线_0_五至六环_建筑面积', '城市_环线_0_六环外_建筑面积', '城市_环线_0_四至五环_建筑面积', '城市_环线_0_无_建筑面积', '城市_环线_1_无环线_建筑面积', '城市_环线_2_内环内_建筑面积', '城市_环线_2_内环至外环_建筑面积', '城市_环线_2_外环外_建筑面积', '城市_环线_2_无_建筑面积', '城市_环线_3_中环至外环_建筑面积', '城市_环线_3_内环内_建筑面积', '城市_环线_3_内环至中环_建筑面积', '城市_环线_3_外环外_建筑面积', '城市_环线_3_无_建筑面积', '城市_环线_4_一环内_建筑面积', '城市_环线_4_一至二环_建筑面积', '城市_环线_4_三至四环_建筑面积', '城市_环线_4_二至三环_建筑面积', '城市_环线_4_四环外_建筑面积', '城市_环线_4_无_建筑面积', '城市_环线_5_无环线_建筑面积', '城市_环线_6_一环内_建筑面积', '城市_环线_6_一至二环_建筑面积', '城市_环线_6_三环外_建筑面积', '城市_环线_6_二至三环_建筑面积', '城市_环线_6_无_建筑面积', '户梯比', '室', '厅', '厨', '卫', 'log_价格', '交通_关键词', '配套_关键词', '学区_关键词', '税费_关键词', '户型介绍_是否填写', '关键词匹配数量', '当前楼层', '总楼层', '多层住宅', '黄金楼层', '楼层占比', '楼层占比_平方', '楼层_电梯交互', '楼层_高层交互', '房龄', '面积_房龄', '房龄_平方', '单位租金_平方', '卧室_租金', '面积_租金', '单价', '地理位置评分', '到数据中心距离', '密度_1km', 'knn_5_mean', 'knn_5_std', 'knn_10_mean', 'knn_10_std', '邻域价格排名']\n# 查看特定特征的值\n             地理位置评分    knn_5_mean   到数据中心距离\n56260  13361.794513  13361.794513  4.785937\n3839   49370.007782  49370.007782  5.582157\n24495  12966.463952  12966.463952  8.848840\n80906  17526.283691  17526.283691  4.472848\n8766   31235.990165  31235.990165  5.874099\n"}],"execution_count":46},{"cell_type":"code","metadata":{"id":"FAA034D7683247E8B1CF32F14572E97A","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"enhanced_train = result['train_df']\nenhanced_test=result['test_df']\nenhanced_predict=result['predict_data']","outputs":[],"execution_count":49},{"cell_type":"code","metadata":{"id":"5B3D44829399428FA9FA3CCB3229B24F","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 检查数据集中的列\nprint(\"当前所有列名:\", enhanced_predict.columns.tolist())","outputs":[{"output_type":"stream","name":"stdout","text":"当前所有列名: ['城市', '区域', '板块', '环线', '小区名称', '房屋户型', '所在楼层', '建筑面积', '梯户比例', '配备电梯', '房屋优势', '核心卖点', '户型介绍', '周边配套', '交通出行', 'lon', 'lat', 'ID', '匹配小区', '建筑年代', '房屋总数', '楼栋总数', '绿 化 率', '容 积 率', '物 业 费', '停车位', '停车费用', '平均每平米月租金', '交易年份', '建筑结构编码_混合结构', '建筑结构编码_钢混结构', '装修编码_毛坯', '装修编码_简装', '装修编码_精装', '房屋数量', '楼栋数量', '绿化率', '容积率', '物业费', '停车位数量', '停车费', 'poly_房屋数量', 'poly_楼栋数量', 'poly_绿化率', 'poly_容积率', 'poly_物业费', 'poly_停车位数量', 'poly_停车费', 'poly_房屋数量^2', 'poly_房屋数量 楼栋数量', 'poly_房屋数量 绿化率', 'poly_房屋数量 容积率', 'poly_房屋数量 物业费', 'poly_房屋数量 停车位数量', 'poly_房屋数量 停车费', 'poly_楼栋数量^2', 'poly_楼栋数量 绿化率', 'poly_楼栋数量 容积率', 'poly_楼栋数量 物业费', 'poly_楼栋数量 停车位数量', 'poly_楼栋数量 停车费', 'poly_绿化率^2', 'poly_绿化率 容积率', 'poly_绿化率 物业费', 'poly_绿化率 停车位数量', 'poly_绿化率 停车费', 'poly_容积率^2', 'poly_容积率 物业费', 'poly_容积率 停车位数量', 'poly_容积率 停车费', 'poly_物业费^2', 'poly_物业费 停车位数量', 'poly_物业费 停车费', 'poly_停车位数量^2', 'poly_停车位数量 停车费', 'poly_停车费^2', 'area_scaled', 'area_squared', '主要朝向_北', '主要朝向_南', '主要朝向_西', '环线_填补', '城市_环线_0_二环内', '城市_环线_0_二至三环', '城市_环线_0_五至六环', '城市_环线_0_六环外', '城市_环线_0_四至五环', '城市_环线_0_无', '城市_环线_1_无环线', '城市_环线_2_内环内', '城市_环线_2_内环至外环', '城市_环线_2_外环外', '城市_环线_2_无', '城市_环线_3_中环至外环', '城市_环线_3_内环内', '城市_环线_3_内环至中环', '城市_环线_3_外环外', '城市_环线_3_无', '城市_环线_4_一环内', '城市_环线_4_一至二环', '城市_环线_4_三至四环', '城市_环线_4_二至三环', '城市_环线_4_四环外', '城市_环线_4_无', '城市_环线_5_无环线', '城市_环线_6_一环内', '城市_环线_6_一至二环', '城市_环线_6_三环外', '城市_环线_6_二至三环', '城市_环线_6_无', '城市_环线_0_二环内_建筑面积', '城市_环线_0_二至三环_建筑面积', '城市_环线_0_五至六环_建筑面积', '城市_环线_0_六环外_建筑面积', '城市_环线_0_四至五环_建筑面积', '城市_环线_0_无_建筑面积', '城市_环线_1_无环线_建筑面积', '城市_环线_2_内环内_建筑面积', '城市_环线_2_内环至外环_建筑面积', '城市_环线_2_外环外_建筑面积', '城市_环线_2_无_建筑面积', '城市_环线_3_中环至外环_建筑面积', '城市_环线_3_内环内_建筑面积', '城市_环线_3_内环至中环_建筑面积', '城市_环线_3_外环外_建筑面积', '城市_环线_3_无_建筑面积', '城市_环线_4_一环内_建筑面积', '城市_环线_4_一至二环_建筑面积', '城市_环线_4_三至四环_建筑面积', '城市_环线_4_二至三环_建筑面积', '城市_环线_4_四环外_建筑面积', '城市_环线_4_无_建筑面积', '城市_环线_5_无环线_建筑面积', '城市_环线_6_一环内_建筑面积', '城市_环线_6_一至二环_建筑面积', '城市_环线_6_三环外_建筑面积', '城市_环线_6_二至三环_建筑面积', '城市_环线_6_无_建筑面积', '户梯比', '室', '厅', '厨', '卫', '交通_得分', '配套_得分', '学区_得分', '税费_得分', '描述丰富度', '核心卖点_特征', '房屋优势_长度', '核心卖点_长度', '周边配套_长度', '交通出行_长度', '综合文本得分', '房屋优势_长度_log', '核心卖点_长度_log', '周边配套_长度_log', '交通出行_长度_log', '综合文本得分_scaled', '描述丰富度_分档', '核心卖点_特征_encoded', '学区交通_交互', '配套描述_交互', '交通_关键词', '配套_关键词', '学区_关键词', '税费_关键词', '户型介绍_是否填写', '关键词匹配数量', '当前楼层', '总楼层', '多层住宅', '黄金楼层', '楼层占比', '楼层占比_平方', '楼层_电梯交互', '楼层_高层交互', '房龄', '面积_房龄', '房龄_平方', '单位租金_平方', '卧室_租金', '面积_租金', '地理位置评分', '到数据中心距离', '密度_1km', 'knn_5_mean', 'knn_5_std', 'knn_10_mean', 'knn_10_std', 'cluster_id', 'cluster_avg_price']\n"}],"execution_count":51},{"cell_type":"markdown","metadata":{"_id":"21D7F981B6C74BD1A25A87CBD59C9D51","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"23754E855BCF4EE29D8AA7FD6139ECF3","notebookId":"68385c9b74983da03017c046","collapsed":false},"source":"# 训练模型"},{"cell_type":"code","metadata":{"_id":"4CEC0D01FA7A49CEAC1514F9A34568C2","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CEA590BDB7F64236A5F91D78A0BADFD4","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"# 检查 DataFrame 是否包含稀疏列\nif any(pd.api.types.is_sparse(enhanced_train[col]) for col in enhanced_train.columns):\n    print(\"DataFrame 包含稀疏列！\")\nelse:\n    print(\"DataFrame 不含稀疏列，可能是普通数值数据。\")\n\n# 如果数据是 pandas.DataFrame 且含稀疏列\nfor col in enhanced_train.columns:\n    if pd.api.types.is_sparse(enhanced_train[col]):\n       enhanced_train[col] = enhanced_train[col].sparse.to_dense()  # 逐列转换\nfor col in enhanced_test.columns:\n    if pd.api.types.is_sparse(enhanced_test[col]):\n       enhanced_test[col] = enhanced_test[col].sparse.to_dense()\nfor col in enhanced_predict.columns:\n    if pd.api.types.is_sparse(enhanced_predict[col]):\n         enhanced_predict[col] = enhanced_predict[col].sparse.to_dense()","outputs":[{"output_type":"stream","name":"stdout","text":"DataFrame 包含稀疏列！\n"}],"execution_count":53},{"cell_type":"code","metadata":{"_id":"4A535A6B848940CDA13C7C8021CA7706","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CF33EA4DF5ED4F1CAE1BEE46E0363F4F","notebookId":"68385c9b74983da03017c046","trusted":true},"source":"selected_features =['区域','板块','area_scaled', 'area_squared','配备电梯', '平均每平米月租金',\n               '室', '厅', '厨', '卫', '房龄_平方','户梯比',\n               '房龄', '楼层占比','面积_房龄','楼层_电梯交互', '楼层_高层交互','交通_得分', '配套_得分', \n               '学区_得分', '税费_得分', '描述丰富度', '房屋优势_长度', '核心卖点_长度', '周边配套_长度', \n               '交通出行_长度', '综合文本得分', '房屋优势_长度_log', '核心卖点_长度_log', '周边配套_长度_log', \n               '交通出行_长度_log', '综合文本得分_scaled', '描述丰富度_分档', '核心卖点_特征_encoded', '学区交通_交互', '配套描述_交互',\n               '建筑结构编码_混合结构', '建筑结构编码_钢混结构', '装修编码_简装', '装修编码_精装','装修编码_毛坯',\n                *city_ring_columns,*[f\"{col}_建筑面积\" for col in city_ring_columns],\n               '绿化率', '容积率', '物业费', '停车位数量', '停车费', 'poly_房屋数量', 'poly_楼栋数量', 'poly_绿化率', \n               'poly_容积率', 'poly_物业费', 'poly_停车位数量', 'poly_停车费', 'poly_房屋数量^2', 'poly_房屋数量 楼栋数量', \n               'poly_房屋数量 绿化率', 'poly_房屋数量 容积率', 'poly_房屋数量 物业费', 'poly_房屋数量 停车位数量', 'poly_房屋数量 停车费', \n               'poly_楼栋数量^2', 'poly_楼栋数量 绿化率', 'poly_楼栋数量 容积率', 'poly_楼栋数量 物业费', 'poly_楼栋数量 停车位数量', \n               'poly_楼栋数量 停车费', 'poly_绿化率^2', 'poly_绿化率 容积率', 'poly_绿化率 物业费', 'poly_绿化率 停车位数量',\n               'poly_绿化率 停车费', 'poly_容积率^2', 'poly_容积率 物业费', 'poly_容积率 停车位数量', 'poly_容积率 停车费', \n               'poly_物业费^2', 'poly_物业费 停车位数量', 'poly_物业费 停车费', 'poly_停车位数量^2', 'poly_停车位数量 停车费', 'poly_停车费^2',\n               '主要朝向_北', '主要朝向_南', '主要朝向_西','单位租金_平方', '卧室_租金', '面积_租金','地理位置评分', '到数据中心距离', '密度_1km',\n               'knn_5_mean', 'knn_5_std', 'knn_10_mean', 'knn_10_std', 'cluster_id', 'cluster_avg_price']\n\n# 因变量\ny_train = enhanced_train['log_价格']\ny_test = enhanced_test['log_价格']\n\nX_train = enhanced_train[selected_features]\nX_test = enhanced_test[selected_features]\n\n#新增\nX_train = pd.DataFrame(X_train, columns=selected_features) \nX_test = pd.DataFrame(X_test, columns=selected_features)\n\n# 定义评估函数同时计算MAE和RMSE\ndef evaluate_model(model, X_train, y_train, X_test, y_test):\n    # 训练模型\n    model.fit(X_train, y_train)\n    \n    # 预测\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    # 计算原始价格的评估指标 (将log转换回原始价格)\n    y_train_original = np.exp(y_train)\n    y_train_pred_original = np.exp(y_train_pred)\n    y_test_original = np.exp(y_test)\n    y_test_pred_original = np.exp(y_test_pred)\n\n    print(\"是否存在 NaN/Inf 或值爆炸？\")\n    print(\"y_train_original 中 NaN：\", np.any(np.isnan(y_train_original)))\n    print(\"y_train_pred_original 中 NaN：\", np.any(np.isnan(y_train_pred_original)))\n    print(\"y_train_pred_original 最大值：\", np.max(y_train_pred_original))\n    print(\"y_train_pred 原始最大 log 值：\", np.max(y_train_pred))\n\n    \n    # 训练集指标\n    train_mae = mean_absolute_error(y_train_original, y_train_pred_original)\n    train_rmse = np.sqrt(mean_squared_error(y_train_original, y_train_pred_original))\n    \n    # 测试集指标\n    test_mae = mean_absolute_error(y_test_original, y_test_pred_original)\n    test_rmse = np.sqrt(mean_squared_error(y_test_original, y_test_pred_original))\n\n    # 输出评估指标\n    print(f\"Train MAE: {train_mae:.2f}, Train RMSE: {train_rmse:.2f}\")\n    print(f\"Test MAE: {test_mae:.2f}, Test RMSE: {test_rmse:.2f}\")\n\n    return {\n        \"train_mae\": train_mae,\n        \"train_rmse\": train_rmse,\n        \"test_mae\": test_mae,\n        \"test_rmse\": test_rmse\n    }","outputs":[],"execution_count":56},{"cell_type":"code","metadata":{"id":"8C39E8DA5F514D6AA044CE7951ACC256","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# XGboost 基础版本\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\nimport pandas as pd\n\nxgb_model = xgb.XGBRegressor(\n    n_estimators=1000,\n    max_depth=7,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42\n)\n\n# 使用你现有的评估函数\nresults = evaluate_model(xgb_model, X_train, y_train, X_test, y_test)","outputs":[{"output_type":"stream","name":"stdout","text":"是否存在 NaN/Inf 或值爆炸？\ny_train_original 中 NaN： False\ny_train_pred_original 中 NaN： False\ny_train_pred_original 最大值： 79317600.0\ny_train_pred 原始最大 log 值： 18.18897\nTrain MAE: 83911.24, Train RMSE: 164810.96\nTest MAE: 157161.25, Test RMSE: 519124.44\n"}],"execution_count":57},{"cell_type":"code","metadata":{"id":"C0E7A96E67DC4E799D7AE948313BDC3D","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 假设你已经 fit 过模型\n#xgb_model.fit(X_train, y_train)\n\n# 取特征名\nfeature_names = X_train.columns\n\n# 获得特征重要性分数\nimportances = xgb_model.feature_importances_\n\n# 转为DataFrame，排序查看\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importances\n}).sort_values('Importance', ascending=False)\n\nprint(importance_df.head(20))  # 显示前20个","outputs":[{"output_type":"stream","name":"stdout","text":"          Feature  Importance\n148   knn_10_mean    0.466871\n143        地理位置评分    0.288315\n146    knn_5_mean    0.118367\n2     area_scaled    0.017307\n14          面积_房龄    0.015845\n142         面积_租金    0.013584\n141         卧室_租金    0.013492\n13           楼层占比    0.003557\n9               卫    0.003492\n1              板块    0.003275\n15        楼层_电梯交互    0.001698\n66    城市_环线_6_三环外    0.001575\n3    area_squared    0.001446\n45   城市_环线_0_四至五环    0.001361\n6               室    0.001198\n150    cluster_id    0.001011\n55    城市_环线_3_外环外    0.000943\n60   城市_环线_4_二至三环    0.000882\n149    knn_10_std    0.000873\n147     knn_5_std    0.000825\n"}],"execution_count":58},{"cell_type":"code","metadata":{"id":"B522EDC4BD8F48A1B53F6666654D73DE","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 1. 定义优化后的模型\noptimized_xgbmodel = xgb.XGBRegressor(\n    n_estimators=710,\n    max_depth=7,\n    subsample=0.772,\n    colsample_bytree=0.858,\n    learning_rate=0.082,\n    random_state=42\n)\n\n# 2. 训练并评估\noptimized_results = evaluate_model(optimized_xgbmodel, X_train, y_train, X_test, y_test)\n\n# 3. 对比基础版\nprint(\"基础版结果:\", results)\nprint(\"优化版结果:\", optimized_results)","outputs":[{"output_type":"stream","name":"stdout","text":"是否存在 NaN/Inf 或值爆炸？\ny_train_original 中 NaN： False\ny_train_pred_original 中 NaN： False\ny_train_pred_original 最大值： 77940130.0\ny_train_pred 原始最大 log 值： 18.171452\nTrain MAE: 103785.58, Train RMSE: 213961.37\nTest MAE: 160714.72, Test RMSE: 553815.19\n基础版结果: {'train_mae': 83911.2417835258, 'train_rmse': 164810.9647245222, 'test_mae': 157161.2453783729, 'test_rmse': 519124.4441576613}\n优化版结果: {'train_mae': 103785.58096965612, 'train_rmse': 213961.3724145273, 'test_mae': 160714.72009531755, 'test_rmse': 553815.188282513}\n"}],"execution_count":63},{"cell_type":"code","metadata":{"id":"B266B700494F4DCBA8450869C34939F3","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import numpy as np\nimport pandas as pd\n\n# 1. 选取特征\nid_col = enhanced_predict['ID'].copy()\npredict_features = enhanced_predict.drop(columns=['ID'])\npredict_features=enhanced_predict[selected_features]\n# 2. 使用训练好的模型进行预测（模型预测的是 log_价格）\npred_log_price = optimized_xgbmodel.predict(predict_features)\n\n# 3. 将预测的 log_价格 还原为原始价格\npred_price = np.exp(pred_log_price)\n\n# 4. 将预测价格转换为整数\npred_price_int = np.round(pred_price).astype(int)\n\n# 5. 构造结果 DataFrame\nsubmission = pd.DataFrame({\n    'ID': id_col,\n    'Price': pred_price_int\n})\n\n# 6. 保存为 CSV 文件\nsubmission.to_csv(\"prediction_xgboost_K3.csv\", index=False)\nprint(\"预测结果已保存到 prediction_xgboost_K3.csv\")","outputs":[{"output_type":"stream","name":"stdout","text":"预测结果已保存到 prediction_xgboost_K3.csv\n"}],"execution_count":65},{"cell_type":"code","metadata":{"id":"50DB8970929B4AF08AF48C7CCD355ACA","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# XGBoost调参（Optuna）\nimport optuna\nfrom sklearn.metrics import mean_squared_error\n\ndef objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1200),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n        \"random_state\": 42,\n        \"n_jobs\": -1,\n        \"tree_method\": \"hist\"  # 推荐加速训练\n    }\n\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train)\n\n    # 预测（还原价格空间）\n    y_pred_log = model.predict(X_test)\n    y_pred = np.exp(y_pred_log)\n    y_true = np.exp(y_test)\n\n    # 计算 RMSE（原始价格空间）\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    return rmse  # optuna 默认 minimize\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=60)","outputs":[{"output_type":"stream","name":"stderr","text":"[I 2025-06-01 15:06:08,290] A new study created in memory with name: no-name-813c9c0b-2893-4abd-aa2e-77e0948a242e\n[I 2025-06-01 15:06:27,662] Trial 0 finished with value: 594691.652351822 and parameters: {'n_estimators': 781, 'max_depth': 7, 'learning_rate': 0.015350339082741374, 'subsample': 0.8613787076851702, 'colsample_bytree': 0.9133879530269682, 'min_child_weight': 10, 'gamma': 0.1662562108588983}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:06:38,361] Trial 1 finished with value: 624404.7074830135 and parameters: {'n_estimators': 778, 'max_depth': 4, 'learning_rate': 0.03229064764655059, 'subsample': 0.8812536071850743, 'colsample_bytree': 0.8333827947386697, 'min_child_weight': 10, 'gamma': 0.04283411824139938}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:06:49,497] Trial 2 finished with value: 749818.3262821162 and parameters: {'n_estimators': 1003, 'max_depth': 7, 'learning_rate': 0.03625358481003018, 'subsample': 0.7017421226391961, 'colsample_bytree': 0.9542729938564379, 'min_child_weight': 2, 'gamma': 3.7109454591869664}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:07:01,239] Trial 3 finished with value: 710033.8744524514 and parameters: {'n_estimators': 1088, 'max_depth': 4, 'learning_rate': 0.028184496155493684, 'subsample': 0.7196578511506977, 'colsample_bytree': 0.8932500046685617, 'min_child_weight': 6, 'gamma': 1.982894791529188}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:07:12,245] Trial 4 finished with value: 661524.3838412093 and parameters: {'n_estimators': 939, 'max_depth': 3, 'learning_rate': 0.02091494155343932, 'subsample': 0.6905512284433231, 'colsample_bytree': 0.8321081053380305, 'min_child_weight': 1, 'gamma': 0.5713675410805857}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:07:24,315] Trial 5 finished with value: 691783.6740542017 and parameters: {'n_estimators': 942, 'max_depth': 4, 'learning_rate': 0.014743831417907166, 'subsample': 0.8289651321668907, 'colsample_bytree': 0.9025577147162523, 'min_child_weight': 1, 'gamma': 1.3858514831291617}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:07:35,007] Trial 6 finished with value: 803476.0072246613 and parameters: {'n_estimators': 583, 'max_depth': 7, 'learning_rate': 0.010519170025305663, 'subsample': 0.6559230091914462, 'colsample_bytree': 0.8371973563537345, 'min_child_weight': 5, 'gamma': 3.833502661859077}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:07:47,064] Trial 7 finished with value: 613771.4934203268 and parameters: {'n_estimators': 1026, 'max_depth': 5, 'learning_rate': 0.03551335627609594, 'subsample': 0.8002043260172419, 'colsample_bytree': 0.8099076742702771, 'min_child_weight': 3, 'gamma': 0.33420890449867513}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:07:58,232] Trial 8 finished with value: 662164.1310344343 and parameters: {'n_estimators': 820, 'max_depth': 6, 'learning_rate': 0.020848247119519617, 'subsample': 0.8956992905592911, 'colsample_bytree': 0.6343688064152363, 'min_child_weight': 10, 'gamma': 0.8838574119609266}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:08:13,943] Trial 9 finished with value: 676265.6848521152 and parameters: {'n_estimators': 1127, 'max_depth': 4, 'learning_rate': 0.010092119171554144, 'subsample': 0.7044259233798995, 'colsample_bytree': 0.7077507086718033, 'min_child_weight': 5, 'gamma': 0.869656420497168}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:08:20,473] Trial 10 finished with value: 715936.6359095654 and parameters: {'n_estimators': 638, 'max_depth': 10, 'learning_rate': 0.12544408182438457, 'subsample': 0.9642643186359579, 'colsample_bytree': 0.9934043498811043, 'min_child_weight': 8, 'gamma': 2.964484263463633}. Best is trial 0 with value: 594691.652351822.\n[I 2025-06-01 15:08:34,615] Trial 11 finished with value: 559180.3979142435 and parameters: {'n_estimators': 726, 'max_depth': 9, 'learning_rate': 0.07989797257071371, 'subsample': 0.8011509623504367, 'colsample_bytree': 0.7329131789679366, 'min_child_weight': 3, 'gamma': 0.03302167341360199}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:08:41,653] Trial 12 finished with value: 784951.3810106008 and parameters: {'n_estimators': 732, 'max_depth': 9, 'learning_rate': 0.08602247296785558, 'subsample': 0.8656749396419773, 'colsample_bytree': 0.7167157744814681, 'min_child_weight': 7, 'gamma': 4.762179130286472}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:08:46,912] Trial 13 finished with value: 697578.5682080401 and parameters: {'n_estimators': 519, 'max_depth': 8, 'learning_rate': 0.195751186059562, 'subsample': 0.7703124036461169, 'colsample_bytree': 0.7207651665036041, 'min_child_weight': 4, 'gamma': 1.7565545607465398}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:08:54,016] Trial 14 finished with value: 715292.8101832739 and parameters: {'n_estimators': 661, 'max_depth': 9, 'learning_rate': 0.06857400531933498, 'subsample': 0.9705120509038885, 'colsample_bytree': 0.7541116481177286, 'min_child_weight': 8, 'gamma': 2.4915546055748017}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:09:06,166] Trial 15 finished with value: 599725.5286973853 and parameters: {'n_estimators': 873, 'max_depth': 8, 'learning_rate': 0.2692835022245571, 'subsample': 0.9196744246375508, 'colsample_bytree': 0.6683536534321679, 'min_child_weight': 3, 'gamma': 0.012180374122945231}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:09:14,126] Trial 16 finished with value: 679073.9158727869 and parameters: {'n_estimators': 705, 'max_depth': 10, 'learning_rate': 0.05233557765825275, 'subsample': 0.7655390443479498, 'colsample_bytree': 0.6010334477809983, 'min_child_weight': 9, 'gamma': 1.3146989295015121}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:09:23,072] Trial 17 finished with value: 664453.9629628696 and parameters: {'n_estimators': 861, 'max_depth': 6, 'learning_rate': 0.10535212744354816, 'subsample': 0.8367660327924972, 'colsample_bytree': 0.8905793131307745, 'min_child_weight': 4, 'gamma': 1.1268443189139916}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:09:31,285] Trial 18 finished with value: 724771.1214378781 and parameters: {'n_estimators': 770, 'max_depth': 8, 'learning_rate': 0.05305262405980241, 'subsample': 0.7639736277433757, 'colsample_bytree': 0.772250734762228, 'min_child_weight': 6, 'gamma': 2.5227262642452164}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:09:42,940] Trial 19 finished with value: 708274.3931964397 and parameters: {'n_estimators': 1188, 'max_depth': 9, 'learning_rate': 0.14955245931351216, 'subsample': 0.6204785512887478, 'colsample_bytree': 0.938400797474114, 'min_child_weight': 3, 'gamma': 1.883056081374367}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:09:50,196] Trial 20 finished with value: 637597.4354114671 and parameters: {'n_estimators': 585, 'max_depth': 7, 'learning_rate': 0.07289363034018274, 'subsample': 0.9289003989680799, 'colsample_bytree': 0.7878102421080228, 'min_child_weight': 7, 'gamma': 0.4221077171483505}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:10:03,192] Trial 21 finished with value: 585256.7162988697 and parameters: {'n_estimators': 887, 'max_depth': 8, 'learning_rate': 0.23933725857915505, 'subsample': 0.9205905117446718, 'colsample_bytree': 0.6737723922341624, 'min_child_weight': 3, 'gamma': 0.010874345393057754}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:10:12,101] Trial 22 finished with value: 674926.481879913 and parameters: {'n_estimators': 932, 'max_depth': 8, 'learning_rate': 0.29610754169035236, 'subsample': 0.8418770365388714, 'colsample_bytree': 0.6632033401723094, 'min_child_weight': 2, 'gamma': 0.5403236741448887}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:10:21,939] Trial 23 finished with value: 577132.3324496484 and parameters: {'n_estimators': 807, 'max_depth': 9, 'learning_rate': 0.14923559312418996, 'subsample': 0.9434365912577333, 'colsample_bytree': 0.7416762374982167, 'min_child_weight': 4, 'gamma': 0.05978191542868835}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:10:30,541] Trial 24 finished with value: 670732.0520302827 and parameters: {'n_estimators': 872, 'max_depth': 9, 'learning_rate': 0.1628323507803154, 'subsample': 0.9896674008430522, 'colsample_bytree': 0.7437113743663769, 'min_child_weight': 4, 'gamma': 0.801645311689698}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:10:39,908] Trial 25 finished with value: 562604.8492750224 and parameters: {'n_estimators': 713, 'max_depth': 10, 'learning_rate': 0.21323474287764002, 'subsample': 0.9378499298501375, 'colsample_bytree': 0.67968674691577, 'min_child_weight': 2, 'gamma': 0.03272733911769775}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:10:46,941] Trial 26 finished with value: 646172.9578884057 and parameters: {'n_estimators': 698, 'max_depth': 10, 'learning_rate': 0.18624442892209445, 'subsample': 0.9498165815647108, 'colsample_bytree': 0.6893466287796983, 'min_child_weight': 2, 'gamma': 0.6343680911624202}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:10:54,354] Trial 27 finished with value: 685196.2000282406 and parameters: {'n_estimators': 633, 'max_depth': 10, 'learning_rate': 0.11885599923790074, 'subsample': 0.9956521612559578, 'colsample_bytree': 0.6313050985323345, 'min_child_weight': 1, 'gamma': 1.4637685150889652}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:11:02,741] Trial 28 finished with value: 657664.6780752293 and parameters: {'n_estimators': 736, 'max_depth': 9, 'learning_rate': 0.09599078371441225, 'subsample': 0.7945068562599207, 'colsample_bytree': 0.7444609012727293, 'min_child_weight': 4, 'gamma': 1.0490974245528901}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:11:12,091] Trial 29 finished with value: 583765.7273064324 and parameters: {'n_estimators': 813, 'max_depth': 10, 'learning_rate': 0.2170053491238919, 'subsample': 0.8993674483645511, 'colsample_bytree': 0.6395692777186106, 'min_child_weight': 2, 'gamma': 0.24441470728060988}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:11:18,828] Trial 30 finished with value: 612484.9711508463 and parameters: {'n_estimators': 581, 'max_depth': 9, 'learning_rate': 0.14383711233766994, 'subsample': 0.9478395383497655, 'colsample_bytree': 0.7074785879882064, 'min_child_weight': 5, 'gamma': 0.27090225892013964}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:11:27,135] Trial 31 finished with value: 600837.0384191921 and parameters: {'n_estimators': 821, 'max_depth': 10, 'learning_rate': 0.21603313936317148, 'subsample': 0.9118402784368436, 'colsample_bytree': 0.6366562000800926, 'min_child_weight': 2, 'gamma': 0.31649667265548787}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:11:41,629] Trial 32 finished with value: 600049.9668788402 and parameters: {'n_estimators': 798, 'max_depth': 10, 'learning_rate': 0.17931564738562517, 'subsample': 0.8784992591670933, 'colsample_bytree': 0.6152700220526751, 'min_child_weight': 2, 'gamma': 0.011438856394809394}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:11:49,439] Trial 33 finished with value: 663836.601824854 and parameters: {'n_estimators': 763, 'max_depth': 10, 'learning_rate': 0.23895912945016218, 'subsample': 0.8921738642971166, 'colsample_bytree': 0.655205906233558, 'min_child_weight': 3, 'gamma': 0.34312853295632084}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:11:56,836] Trial 34 finished with value: 643694.0803352684 and parameters: {'n_estimators': 683, 'max_depth': 9, 'learning_rate': 0.13106659165418424, 'subsample': 0.9439620319340943, 'colsample_bytree': 0.6895466658972953, 'min_child_weight': 2, 'gamma': 0.6610155274734462}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:12:06,587] Trial 35 finished with value: 595093.7710588251 and parameters: {'n_estimators': 749, 'max_depth': 10, 'learning_rate': 0.07714172367192575, 'subsample': 0.8583482368906954, 'colsample_bytree': 0.7268674236816256, 'min_child_weight': 1, 'gamma': 0.21737119730829638}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:12:15,574] Trial 36 finished with value: 751298.3944718082 and parameters: {'n_estimators': 803, 'max_depth': 9, 'learning_rate': 0.045747156236787344, 'subsample': 0.9012873285919956, 'colsample_bytree': 0.7748027621232217, 'min_child_weight': 4, 'gamma': 3.668895507856363}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:12:24,848] Trial 37 finished with value: 697037.2236219816 and parameters: {'n_estimators': 987, 'max_depth': 8, 'learning_rate': 0.10916995458764485, 'subsample': 0.9730200886672948, 'colsample_bytree': 0.6881433937624261, 'min_child_weight': 1, 'gamma': 1.6238822687005814}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:12:33,515] Trial 38 finished with value: 691357.4869874505 and parameters: {'n_estimators': 910, 'max_depth': 10, 'learning_rate': 0.16370366339636758, 'subsample': 0.810401005948565, 'colsample_bytree': 0.8655694553330614, 'min_child_weight': 3, 'gamma': 1.103813256669113}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:12:41,570] Trial 39 finished with value: 648678.1119836358 and parameters: {'n_estimators': 837, 'max_depth': 7, 'learning_rate': 0.2078434784259198, 'subsample': 0.740432355539275, 'colsample_bytree': 0.6470638500610463, 'min_child_weight': 2, 'gamma': 0.6164801654671851}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:12:48,675] Trial 40 finished with value: 737748.7228307899 and parameters: {'n_estimators': 728, 'max_depth': 5, 'learning_rate': 0.24081511732781793, 'subsample': 0.8777834566269883, 'colsample_bytree': 0.8068802017994523, 'min_child_weight': 5, 'gamma': 2.144968625233925}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:12:59,284] Trial 41 finished with value: 595583.7415969177 and parameters: {'n_estimators': 885, 'max_depth': 8, 'learning_rate': 0.27044096096539516, 'subsample': 0.9169875020637743, 'colsample_bytree': 0.6781685712775679, 'min_child_weight': 3, 'gamma': 0.03214155344224269}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:13:07,710] Trial 42 finished with value: 584637.3356545452 and parameters: {'n_estimators': 795, 'max_depth': 9, 'learning_rate': 0.24167937935367922, 'subsample': 0.9331284688772978, 'colsample_bytree': 0.6171667835176317, 'min_child_weight': 3, 'gamma': 0.19293251151812318}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:13:15,850] Trial 43 finished with value: 622907.1680189341 and parameters: {'n_estimators': 766, 'max_depth': 9, 'learning_rate': 0.1494020495225582, 'subsample': 0.9341822579434323, 'colsample_bytree': 0.6181404491839436, 'min_child_weight': 4, 'gamma': 0.4200754599737018}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:13:24,058] Trial 44 finished with value: 687999.2816129739 and parameters: {'n_estimators': 842, 'max_depth': 9, 'learning_rate': 0.2059513763880449, 'subsample': 0.9742588021798847, 'colsample_bytree': 0.7031759626506868, 'min_child_weight': 2, 'gamma': 0.9044499625388838}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:13:32,404] Trial 45 finished with value: 613580.2200167803 and parameters: {'n_estimators': 786, 'max_depth': 10, 'learning_rate': 0.17732193393397583, 'subsample': 0.8543102872638919, 'colsample_bytree': 0.7328987566044816, 'min_child_weight': 1, 'gamma': 0.29196356924893546}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:13:40,695] Trial 46 finished with value: 671606.11211231 and parameters: {'n_estimators': 653, 'max_depth': 3, 'learning_rate': 0.02861918646162376, 'subsample': 0.8200512918298518, 'colsample_bytree': 0.7627990639877298, 'min_child_weight': 3, 'gamma': 0.7154445180991283}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:13:47,180] Trial 47 finished with value: 782595.0492933198 and parameters: {'n_estimators': 615, 'max_depth': 9, 'learning_rate': 0.06433522824102623, 'subsample': 0.9628720327731785, 'colsample_bytree': 0.6011240332650272, 'min_child_weight': 5, 'gamma': 4.956961571983003}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:13:54,237] Trial 48 finished with value: 731605.447852052 and parameters: {'n_estimators': 710, 'max_depth': 10, 'learning_rate': 0.08936723486187244, 'subsample': 0.6775036283656896, 'colsample_bytree': 0.644842215974784, 'min_child_weight': 4, 'gamma': 3.3584694884271027}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:14:00,702] Trial 49 finished with value: 789110.5356385228 and parameters: {'n_estimators': 683, 'max_depth': 9, 'learning_rate': 0.2794493867217202, 'subsample': 0.8975341439178477, 'colsample_bytree': 0.6210859151967348, 'min_child_weight': 2, 'gamma': 4.333580125085289}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:14:10,802] Trial 50 finished with value: 655454.3249633975 and parameters: {'n_estimators': 818, 'max_depth': 8, 'learning_rate': 0.03998271522056596, 'subsample': 0.9370186554974328, 'colsample_bytree': 0.700882479006478, 'min_child_weight': 3, 'gamma': 0.9603002033104633}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:14:20,293] Trial 51 finished with value: 575051.4173787631 and parameters: {'n_estimators': 910, 'max_depth': 8, 'learning_rate': 0.2271125789033918, 'subsample': 0.9137436546024243, 'colsample_bytree': 0.6676479944224244, 'min_child_weight': 3, 'gamma': 0.12462842842763985}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:14:29,742] Trial 52 finished with value: 591257.8318157386 and parameters: {'n_estimators': 967, 'max_depth': 9, 'learning_rate': 0.22328560317305252, 'subsample': 0.9580460659319578, 'colsample_bytree': 0.6591171583369836, 'min_child_weight': 3, 'gamma': 0.19096309696143002}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:14:39,643] Trial 53 finished with value: 629071.1801453602 and parameters: {'n_estimators': 1078, 'max_depth': 7, 'learning_rate': 0.29964283655853835, 'subsample': 0.9073094441642086, 'colsample_bytree': 0.6770334761109942, 'min_child_weight': 4, 'gamma': 0.47097258982032997}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:14:48,853] Trial 54 finished with value: 615742.4487306037 and parameters: {'n_estimators': 916, 'max_depth': 10, 'learning_rate': 0.25616824677636163, 'subsample': 0.8854688988632982, 'colsample_bytree': 0.831605592543787, 'min_child_weight': 2, 'gamma': 0.17096543343788612}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:14:56,740] Trial 55 finished with value: 645737.0884038511 and parameters: {'n_estimators': 840, 'max_depth': 8, 'learning_rate': 0.1924455508049127, 'subsample': 0.9263286151251306, 'colsample_bytree': 0.7160018697568329, 'min_child_weight': 3, 'gamma': 0.5202328557787445}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:15:06,888] Trial 56 finished with value: 678265.5267326381 and parameters: {'n_estimators': 1049, 'max_depth': 9, 'learning_rate': 0.13786984642938674, 'subsample': 0.8684638143706823, 'colsample_bytree': 0.7899370031650959, 'min_child_weight': 1, 'gamma': 1.2167718746960747}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:15:30,366] Trial 57 finished with value: 594586.7427036542 and parameters: {'n_estimators': 956, 'max_depth': 10, 'learning_rate': 0.014648454756702952, 'subsample': 0.9869903129207099, 'colsample_bytree': 0.6496010091077582, 'min_child_weight': 6, 'gamma': 0.15525298395322415}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:15:38,278] Trial 58 finished with value: 649432.693985288 and parameters: {'n_estimators': 796, 'max_depth': 6, 'learning_rate': 0.1687998045858729, 'subsample': 0.9526388949543958, 'colsample_bytree': 0.6336631359424125, 'min_child_weight': 3, 'gamma': 0.793615926307458}. Best is trial 11 with value: 559180.3979142435.\n[I 2025-06-01 15:15:45,797] Trial 59 finished with value: 658599.2148339126 and parameters: {'n_estimators': 721, 'max_depth': 9, 'learning_rate': 0.1198235560860571, 'subsample': 0.6126937328162285, 'colsample_bytree': 0.6017339201233859, 'min_child_weight': 4, 'gamma': 0.4546534340613655}. Best is trial 11 with value: 559180.3979142435.\n"}],"execution_count":59},{"cell_type":"code","metadata":{"id":"36E5D63971834346BE655B2598734D03","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"from sklearn.model_selection import KFold\n\ndef fast_optuna_objective_cv(trial, X_train, y_train, n_splits=3):\n    params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'random_state': 42,\n        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n        'max_depth': trial.suggest_int('max_depth', 5, 12),\n        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.3, log=True),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),\n    }\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=trial.number)\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X_train):\n        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n        model = xgb.XGBRegressor(**params)\n        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=15, verbose=False)\n        pred = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, pred))\n        cv_scores.append(rmse)\n    return np.mean(cv_scores)\n\ndef run_fast_optuna_cv(X_train, y_train, n_trials=30, n_splits=3):\n    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n    study.optimize(lambda trial: fast_optuna_objective_cv(trial, X_train, y_train, n_splits), n_trials=n_trials, show_progress_bar=True)\n    return study.best_params, study\n\n# 用法\nbest_params, study = run_fast_optuna_cv(X_train, y_train, n_trials=30, n_splits=3)\nmodel = xgb.XGBRegressor(**best_params)\nmodel.fit(X_train, y_train)\n\n# 训练完成后再用 X_test, y_test 做最终评估\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\ny_test_pred = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\nmae = mean_absolute_error(y_test, y_test_pred)\nprint(f\"Test RMSE: {rmse:.4f}, MAE: {mae:.4f}\")","outputs":[{"output_type":"stream","name":"stderr","text":"[I 2025-06-01 15:23:49,828] A new study created in memory with name: no-name-2dda68ba-2c7f-45e5-8a93-d2fcf736684e\n"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e834e044b84b5aad00ad8ad54ee701"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"[I 2025-06-01 15:25:36,782] Trial 0 finished with value: 0.12229841235231476 and parameters: {'n_estimators': 825, 'max_depth': 12, 'learning_rate': 0.14518497439122724, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.8312037280884873}. Best is trial 0 with value: 0.12229841235231476.\n[I 2025-06-01 15:27:58,960] Trial 1 finished with value: 0.12360377383211163 and parameters: {'n_estimators': 693, 'max_depth': 5, 'learning_rate': 0.20880081501089318, 'subsample': 0.8404460046972835, 'colsample_bytree': 0.941614515559209}. Best is trial 0 with value: 0.12229841235231476.\n[I 2025-06-01 15:29:05,508] Trial 2 finished with value: 0.12584550845520026 and parameters: {'n_estimators': 612, 'max_depth': 12, 'learning_rate': 0.19057174434080473, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.8363649934414201}. Best is trial 0 with value: 0.12229841235231476.\n[I 2025-06-01 15:33:19,568] Trial 3 finished with value: 0.11975548889655181 and parameters: {'n_estimators': 710, 'max_depth': 7, 'learning_rate': 0.08283072221077194, 'subsample': 0.7727780074568463, 'colsample_bytree': 0.8582458280396084}. Best is trial 3 with value: 0.11975548889655181.\n[I 2025-06-01 15:38:32,426] Trial 4 finished with value: 0.12070710813012187 and parameters: {'n_estimators': 967, 'max_depth': 6, 'learning_rate': 0.04411829935614202, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.8912139968434072}. Best is trial 3 with value: 0.11975548889655181.\n[I 2025-06-01 15:43:35,626] Trial 5 finished with value: 0.11983845190517602 and parameters: {'n_estimators': 1071, 'max_depth': 6, 'learning_rate': 0.08050384505270129, 'subsample': 0.836965827544817, 'colsample_bytree': 0.8092900825439996}. Best is trial 3 with value: 0.11975548889655181.\n[I 2025-06-01 15:48:34,915] Trial 6 finished with value: 0.12480223924855254 and parameters: {'n_estimators': 965, 'max_depth': 6, 'learning_rate': 0.02385264834812575, 'subsample': 0.9795542149013333, 'colsample_bytree': 0.9931264066149119}. Best is trial 3 with value: 0.11975548889655181.\n[W 2025-06-01 15:52:40,820] Trial 7 failed with parameters: {'n_estimators': 1085, 'max_depth': 7, 'learning_rate': 0.026055614206180563, 'subsample': 0.8736932106048627, 'colsample_bytree': 0.8880304987479203} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_41/1184580206.py\", line 28, in <lambda>\n    study.optimize(lambda trial: fast_optuna_objective_cv(trial, X_train, y_train, n_splits), n_trials=n_trials, show_progress_bar=True)\n  File \"/tmp/ipykernel_41/1184580206.py\", line 20, in fast_optuna_objective_cv\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=15, verbose=False)\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 575, in inner_f\n    return f(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\", line 972, in fit\n    callbacks=callbacks,\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 575, in inner_f\n    return f(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/training.py\", line 181, in train\n    bst.update(dtrain, i, obj)\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 1780, in update\n    dtrain.handle))\nKeyboardInterrupt\n[W 2025-06-01 15:52:40,824] Trial 7 failed with value None.\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_41/1184580206.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 用法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_fast_optuna_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_41/1184580206.py\u001b[0m in \u001b[0;36mrun_fast_optuna_cv\u001b[0;34m(X_train, y_train, n_trials, n_splits)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_fast_optuna_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfast_optuna_objective_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_41/1184580206.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_fast_optuna_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfast_optuna_objective_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_41/1184580206.py\u001b[0m in \u001b[0;36mfast_optuna_objective_cv\u001b[0;34m(trial, X_train, y_train, n_splits)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1778\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1779\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":61},{"cell_type":"code","metadata":{"id":"3F87A4310B9547699248758F0A68FFF6","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"print(\"Best Trial:\")\nprint(study.best_trial.params)\nprint(f\"Best RMSE: {study.best_value:.2f}\")\n\n# 用最优参数重新训练模型\nbest_params = study.best_trial.params\nfinal_model = xgb.XGBRegressor(**best_params)\nfinal_model.fit(X_train, y_train)\n\n# 使用你的 evaluate_model 函数评估\nevaluate_model(final_model, X_train, y_train, X_test, y_test)","outputs":[{"output_type":"stream","name":"stdout","text":"Best Trial:\n{'n_estimators': 942, 'max_depth': 10, 'learning_rate': 0.03959063648664691, 'subsample': 0.9356031520739537, 'colsample_bytree': 0.8959187926548916, 'min_child_weight': 9, 'gamma': 0.0028728519722198776}\nBest RMSE: 528626.70\n"}],"execution_count":62},{"cell_type":"code","metadata":{"id":"5C54647D6D954D0CB4185C2E279D8B06","notebookId":"68385c9b74983da03017c046","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import numpy as np\nimport pandas as pd\n\n# 1. 选取特征\nid_col = processed_predict['ID'].copy()\npredict_features = processed_predict.drop(columns=['ID'])\npredict_features=processed_predict[selected_features]\n# 2. 使用训练好的模型进行预测（模型预测的是 log_价格）\npred_log_price = final_model.predict(predict_features)\n\n# 3. 将预测的 log_价格 还原为原始价格\npred_price = np.exp(pred_log_price)\n\n# 4. 将预测价格转换为整数\npred_price_int = np.round(pred_price).astype(int)\n\n# 5. 构造结果 DataFrame\nsubmission = pd.DataFrame({\n    'ID': id_col,\n    'Price': pred_price_int\n})\n\n# 6. 保存为 CSV 文件\nsubmission.to_csv(\"prediction_xgboost_Koptuna.csv\", index=False)\nprint(\"预测结果已保存到 prediction_xgboost_Koptuna.csv\")","outputs":[],"execution_count":64}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}